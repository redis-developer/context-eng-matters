{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bfa9cdcafeff8",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Module 4: Memory Systems\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 75-90 minutes\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "\n",
    "1. **Understand** why memory is essential for context engineering\n",
    "2. **Implement** working memory for conversation continuity\n",
    "3. **Use** long-term memory for persistent user knowledge\n",
    "4. **Integrate** memory with your Module 2 RAG system\n",
    "5. **Build** a complete memory-enhanced course advisor\n",
    "6. **Combine** all four context types in a unified system\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Recap\n",
    "\n",
    "### **Module 1: The Four Context Types**\n",
    "\n",
    "Recall the four context types from Module 1:\n",
    "\n",
    "1. **System Context** (Static) - Role, instructions, guidelines\n",
    "2. **User Context** (Dynamic, User-Specific) - Profile, preferences, goals\n",
    "3. **Conversation Context** (Dynamic, Session-Specific) - **‚Üê Memory enables this!**\n",
    "4. **Retrieved Context** (Dynamic, Query-Specific) - RAG results\n",
    "\n",
    "### **Module 2: Stateless RAG**\n",
    "\n",
    "Your Module 2 RAG system was **stateless**:\n",
    "\n",
    "```python\n",
    "async def rag_query(query, student_profile):\n",
    "    # 1. Search courses (Retrieved Context)\n",
    "    courses = await course_manager.search_courses(query)\n",
    "\n",
    "    # 2. Assemble context (System + User + Retrieved)\n",
    "    context = assemble_context(system_prompt, student_profile, courses)\n",
    "\n",
    "    # 3. Generate response\n",
    "    response = llm.invoke(context)\n",
    "\n",
    "    # ‚ùå No conversation history stored\n",
    "    # ‚ùå Each query is independent\n",
    "    # ‚ùå Can't reference previous messages\n",
    "```\n",
    "\n",
    "**The Problem:** Every query starts from scratch. No conversation continuity.\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Why Agents Need Memory: The Grounding Problem\n",
    "\n",
    "Before diving into implementation, let's understand the fundamental problem that memory solves.\n",
    "\n",
    "**Grounding** means understanding what users are referring to. Natural conversation is full of references:\n",
    "\n",
    "### **Without Memory:**\n",
    "\n",
    "```\n",
    "User: \"Tell me about CS401\"\n",
    "Agent: \"CS401 is Machine Learning. It covers supervised learning...\"\n",
    "\n",
    "User: \"What are its prerequisites?\"\n",
    "Agent: ‚ùå \"What does 'it' refer to? Please specify which course.\"\n",
    "\n",
    "User: \"The course we just discussed!\"\n",
    "Agent: ‚ùå \"I don't have access to previous messages. Which course?\"\n",
    "```\n",
    "\n",
    "**This is a terrible user experience.**\n",
    "\n",
    "### Types of References That Need Grounding\n",
    "\n",
    "**Pronouns:**\n",
    "- \"it\", \"that course\", \"those\", \"this one\"\n",
    "- \"he\", \"she\", \"they\" (referring to people)\n",
    "\n",
    "**Descriptions:**\n",
    "- \"the easy one\", \"the online course\"\n",
    "- \"my advisor\", \"that professor\"\n",
    "\n",
    "**Implicit context:**\n",
    "- \"Can I take it?\" ‚Üí Take what?\n",
    "- \"When does it start?\" ‚Üí What starts?\n",
    "\n",
    "**Temporal references:**\n",
    "- \"you mentioned\", \"earlier\", \"last time\"\n",
    "\n",
    "### **With Memory:**\n",
    "\n",
    "```\n",
    "User: \"Tell me about CS401\"\n",
    "Agent: \"CS401 is Machine Learning. It covers...\"\n",
    "[Stores: User asked about CS401]\n",
    "\n",
    "User: \"What are its prerequisites?\"\n",
    "Agent: [Checks memory: \"its\" = CS401]\n",
    "Agent: ‚úÖ \"CS401 requires CS201 and MATH301\"\n",
    "\n",
    "User: \"Can I take it?\"\n",
    "Agent: [Checks memory: \"it\" = CS401, checks student transcript]\n",
    "Agent: ‚úÖ \"You've completed CS201 but still need MATH301\"\n",
    "```\n",
    "\n",
    "**Now the conversation flows naturally!**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Two Types of Memory\n",
    "\n",
    "### **1. Working Memory (Session-Scoped)**\n",
    "\n",
    " - **What:** Conversation messages from the current session\n",
    " - **Purpose:** Reference resolution, conversation continuity\n",
    " - **Lifetime:** Persists for the session\n",
    " - **Storage:** Conversation remains accessible when you return to the same session\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Session: session_123\n",
    "Messages:\n",
    "  1. User: \"Tell me about CS401\"\n",
    "  2. Agent: \"CS401 is Machine Learning...\"\n",
    "  3. User: \"What are its prerequisites?\"\n",
    "  4. Agent: \"CS401 requires CS201 and MATH301\"\n",
    "```\n",
    "\n",
    "**Key Point:** Just like ChatGPT or Claude, when you return to a conversation, the working memory is still there. The conversation doesn't disappear!\n",
    "\n",
    "### **2. Long-term Memory (Cross-Session)**\n",
    "\n",
    " - **What:** Persistent knowledge (user preferences, domain facts, business rules)\n",
    " - **Purpose:** Personalization AND consistent application behavior across sessions\n",
    " - **Lifetime:** Permanent (until explicitly deleted)\n",
    " - **Scope:** Can be user-specific OR application-wide\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "**User-Scoped (Personalization):**\n",
    "```\n",
    "User: student_sarah\n",
    "  - \"Prefers online courses over in-person\"\n",
    "  - \"Major: Computer Science, focus on AI/ML\"\n",
    "  - \"Goal: Graduate Spring 2026\"\n",
    "  - \"Completed: CS101, CS201, MATH301\"\n",
    "```\n",
    "\n",
    "**Application-Scoped (Domain Knowledge):**\n",
    "```\n",
    "Domain: course_requirements\n",
    "  - \"CS401 requires CS201 as prerequisite\"\n",
    "  - \"Maximum course load is 18 credits per semester\"\n",
    "  - \"Registration opens 2 weeks before semester start\"\n",
    "  - \"Lab courses require campus attendance\"\n",
    "```\n",
    "\n",
    "### **Comparison: Working vs. Long-term Memory**\n",
    "\n",
    "| Working Memory | Long-term Memory |\n",
    "|----------------|------------------|\n",
    "| **Session-scoped** | **User-scoped OR Application-scoped** |\n",
    "| Current conversation | Important facts, rules, knowledge |\n",
    "| Persists for session | Persists across sessions |\n",
    "| Full message history | Extracted knowledge (user + domain) |\n",
    "| Loaded/saved each turn | Searched when needed |\n",
    "| **Challenge:** Context window limits | **Challenge:** Storage growth |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Setup and Environment\n",
    "\n",
    "Let's set up our environment with the necessary dependencies and connections. We'll build on Module 2's RAG foundation and add memory capabilities.\n",
    "\n",
    "### ‚ö†Ô∏è Prerequisites\n",
    "\n",
    "**Before running this notebook, make sure you have:**\n",
    "\n",
    "1. **Docker Desktop running** - Required for Redis and Agent Memory Server\n",
    "\n",
    "2. **Environment variables** - Create a `.env` file in the project root directory:\n",
    "   ```bash\n",
    "   # Copy the example file (if it exists)\n",
    "   cd ..\n",
    "   # Or create .env manually with:\n",
    "   # OPENAI_API_KEY=your_actual_openai_api_key_here\n",
    "   # REDIS_URL=redis://localhost:6379\n",
    "   # AGENT_MEMORY_URL=http://localhost:8088\n",
    "   ```\n",
    "\n",
    "3. **Start services** - Make sure Redis and Agent Memory Server are running:\n",
    "   ```bash\n",
    "   # Start Redis and Agent Memory Server using docker-compose\n",
    "   cd ..\n",
    "   docker-compose up -d\n",
    "   ```\n",
    "\n",
    "**Note:** Using docker-compose will:\n",
    "- ‚úÖ Start Redis on port 6379\n",
    "- ‚úÖ Start Agent Memory Server on port 8088\n",
    "- ‚úÖ Configure networking between services\n",
    "- ‚úÖ Persist data in Docker volumes\n",
    "\n",
    "If the Memory Server is not available, the notebook will skip memory-related demos but will still run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133103c1578afd1f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf596157ac1dd8",
   "metadata": {},
   "source": [
    "### Automated Setup Check\n",
    "\n",
    "Let's run the setup script to ensure all services are running properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd4ba605c55ba49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:06.570783Z",
     "iopub.status.busy": "2025-12-09T19:56:06.570582Z",
     "iopub.status.idle": "2025-12-09T19:56:06.615146Z",
     "shell.execute_reply": "2025-12-09T19:56:06.614552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if required services are running...\n",
      "\n",
      "‚úÖ Redis is running\n",
      "‚úÖ Agent Memory Server is running\n",
      "\n",
      "If services are not running, start them with:\n",
      "  cd ..\n",
      "  docker-compose up -d\n"
     ]
    }
   ],
   "source": [
    "# Check if services are running\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking if required services are running...\\n\")\n",
    "\n",
    "# Check if Redis is running\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"ps\", \"--filter\", \"name=redis\", \"--format\", \"{{.Names}}\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    if \"redis\" in result.stdout:\n",
    "        print(\"‚úÖ Redis is running\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Redis is not running. Start it with: docker-compose up -d\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not check Redis status: {e}\")\n",
    "\n",
    "# Check if Agent Memory Server is running\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"ps\", \"--filter\", \"name=agent-memory\", \"--format\", \"{{.Names}}\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    if \"agent-memory\" in result.stdout or \"memory\" in result.stdout:\n",
    "        print(\"‚úÖ Agent Memory Server is running\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Agent Memory Server is not running. Start it with: docker-compose up -d\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not check Agent Memory Server status: {e}\")\n",
    "\n",
    "print(\"\\nIf services are not running, start them with:\")\n",
    "print(\"  cd ..\")\n",
    "print(\"  docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56417b09adcbba74",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40618e4a602328df",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "If you haven't already installed the project package, uncomment and run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6af962b1933f405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:06.616905Z",
     "iopub.status.busy": "2025-12-09T19:56:06.616721Z",
     "iopub.status.idle": "2025-12-09T19:56:06.618803Z",
     "shell.execute_reply": "2025-12-09T19:56:06.618348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install the project package\n",
    "# %pip install -q -e ..\n",
    "\n",
    "# Uncomment to install agent-memory-client\n",
    "# %pip install -q agent-memory-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c713c97986b5",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "We'll load environment variables from the `.env` file in the project root directory.\n",
    "\n",
    "**Required variables:**\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key\n",
    "- `REDIS_URL` - Redis connection URL (default: redis://localhost:6379)\n",
    "- `AGENT_MEMORY_URL` - Agent Memory Server URL (default: http://localhost:8088)\n",
    "\n",
    "If you haven't created the `.env` file yet, create it in the project root with your OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3553025fb912d807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:06.619988Z",
     "iopub.status.busy": "2025-12-09T19:56:06.619887Z",
     "iopub.status.idle": "2025-12-09T19:56:06.626576Z",
     "shell.execute_reply": "2025-12-09T19:56:06.626158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n",
      "   REDIS_URL: redis://localhost:6379\n",
      "   AGENT_MEMORY_URL: http://localhost:8088\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Handle both running from workshop/ directory and from project root\n",
    "if Path.cwd().name == \"workshop\":\n",
    "    project_root = Path.cwd().parent\n",
    "else:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from project root\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "AGENT_MEMORY_URL = os.getenv(\"AGENT_MEMORY_URL\", \"http://localhost:8088\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\n",
    "        f\"\"\"‚ùå OPENAI_API_KEY not found!\n",
    "\n",
    "    Please create a .env file at: {env_path.absolute()}\n",
    "\n",
    "    With the following content:\n",
    "    OPENAI_API_KEY=your_openai_api_key\n",
    "    REDIS_URL=redis://localhost:6379\n",
    "    AGENT_MEMORY_URL=http://localhost:8088\n",
    "    \"\"\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\"\"‚úÖ Environment variables loaded\n",
    "   REDIS_URL: {REDIS_URL}\n",
    "   AGENT_MEMORY_URL: {AGENT_MEMORY_URL}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab040509ba57335",
   "metadata": {},
   "source": [
    "### Import Core Libraries\n",
    "\n",
    "We'll import standard Python libraries and async support for our memory operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8fa629dc6e060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:06.627657Z",
     "iopub.status.busy": "2025-12-09T19:56:06.627581Z",
     "iopub.status.idle": "2025-12-09T19:56:06.630348Z",
     "shell.execute_reply": "2025-12-09T19:56:06.629931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core libraries imported\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops (required for Jupyter)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ Core libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170863b81a0d631c",
   "metadata": {},
   "source": [
    "### Import Module 2 Components\n",
    "\n",
    "We're building on Module 2's RAG foundation, so we'll reuse the same components:\n",
    "- `redis_config` - Redis connection and configuration\n",
    "- `HierarchicalCourseManager` - Two-tier course search (summaries + details)\n",
    "- `HierarchicalContextAssembler` - Progressive disclosure context assembly\n",
    "- `StudentProfile` and other models - Data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44dd307e2471f20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:06.631656Z",
     "iopub.status.busy": "2025-12-09T19:56:06.631535Z",
     "iopub.status.idle": "2025-12-09T19:56:08.894025Z",
     "shell.execute_reply": "2025-12-09T19:56:08.893651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Module 2 components imported\n",
      "   HierarchicalCourseManager: Available\n",
      "   HierarchicalContextAssembler: Available\n",
      "   Redis Config: Available\n",
      "   Models: Course, StudentProfile, etc.\n"
     ]
    }
   ],
   "source": [
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "from redis_context_course.hierarchical_manager import HierarchicalCourseManager\n",
    "from redis_context_course.models import (\n",
    "    Course,\n",
    "    CourseFormat,\n",
    "    DifficultyLevel,\n",
    "    Semester,\n",
    "    StudentProfile,\n",
    ")\n",
    "\n",
    "# Import Redis configuration\n",
    "from redis_context_course.redis_config import redis_config\n",
    "\n",
    "print(\"\"\"‚úÖ Module 2 components imported\n",
    "   HierarchicalCourseManager: Available\n",
    "   HierarchicalContextAssembler: Available\n",
    "   Redis Config: Available\n",
    "   Models: Course, StudentProfile, etc.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87d8e7727dfad4",
   "metadata": {},
   "source": [
    "### Import LangChain Components\n",
    "\n",
    "We'll use LangChain for LLM interaction and message handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93f219d77f6c72e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:08.895254Z",
     "iopub.status.busy": "2025-12-09T19:56:08.895126Z",
     "iopub.status.idle": "2025-12-09T19:56:08.896936Z",
     "shell.execute_reply": "2025-12-09T19:56:08.896614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain components imported\n",
      "   ChatOpenAI: Available\n",
      "   Message types: HumanMessage, SystemMessage, AIMessage\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "print(\"\"\"‚úÖ LangChain components imported\n",
    "   ChatOpenAI: Available\n",
    "   Message types: HumanMessage, SystemMessage, AIMessage\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75782c53d70e86",
   "metadata": {},
   "source": [
    "### Import Agent Memory Server Client\n",
    "\n",
    "The Agent Memory Server provides production-ready memory management. If it's not available, we'll note that and continue with limited functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3964be15af5b99a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:08.897974Z",
     "iopub.status.busy": "2025-12-09T19:56:08.897914Z",
     "iopub.status.idle": "2025-12-09T19:56:08.899565Z",
     "shell.execute_reply": "2025-12-09T19:56:08.899158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent Memory Server client available\n",
      "   MemoryAPIClient: Ready\n",
      "   Memory models: WorkingMemory, MemoryMessage, ClientMemoryRecord\n"
     ]
    }
   ],
   "source": [
    "# Import Agent Memory Server client\n",
    "try:\n",
    "    from agent_memory_client import MemoryAPIClient, MemoryClientConfig\n",
    "    from agent_memory_client.models import (\n",
    "        ClientMemoryRecord,\n",
    "        MemoryMessage,\n",
    "        WorkingMemory,\n",
    "    )\n",
    "\n",
    "    MEMORY_SERVER_AVAILABLE = True\n",
    "    print(\"\"\"‚úÖ Agent Memory Server client available\n",
    "   MemoryAPIClient: Ready\n",
    "   Memory models: WorkingMemory, MemoryMessage, ClientMemoryRecord\"\"\")\n",
    "except ImportError:\n",
    "    MEMORY_SERVER_AVAILABLE = False\n",
    "    print(\"\"\"‚ö†Ô∏è  Agent Memory Server not available\n",
    "   Install with: pip install agent-memory-client\n",
    "   Start server: docker-compose up -d\n",
    "   Note: Some demos will be skipped\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aa34146b409fe",
   "metadata": {},
   "source": [
    "### What We Just Did\n",
    "\n",
    "We've successfully set up our environment with all the necessary components:\n",
    "\n",
    "**Imported:**\n",
    "- ‚úÖ Module 2 RAG components (`HierarchicalCourseManager`, `HierarchicalContextAssembler`, `redis_config`, models)\n",
    "- ‚úÖ LangChain for LLM interaction\n",
    "- ‚úÖ Agent Memory Server client (if available)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Building on Module 2's foundation (not starting from scratch)\n",
    "- Using progressive disclosure pattern (summaries ‚Üí details)\n",
    "- Agent Memory Server provides scalable, persistent memory\n",
    "- Same Redis University domain for consistency\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Initialize Components\n",
    "\n",
    "Now let's initialize the components we'll use throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdd177e25e295d",
   "metadata": {},
   "source": [
    "### Initialize Redis Connection\n",
    "\n",
    "First, let's connect to Redis using the same configuration from Module 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba657dd4a1d0857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:08.900590Z",
     "iopub.status.busy": "2025-12-09T19:56:08.900526Z",
     "iopub.status.idle": "2025-12-09T19:56:08.902359Z",
     "shell.execute_reply": "2025-12-09T19:56:08.901941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Redis connection established\n",
      "   URL: redis://localhost:6379\n",
      "   Ready for vector operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Redis connection (redis_config.redis_client is a property)\n",
    "redis_client = redis_config.redis_client\n",
    "\n",
    "print(f\"\"\"‚úÖ Redis connection established\n",
    "   URL: {REDIS_URL}\n",
    "   Ready for vector operations\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0f52a01d8223c",
   "metadata": {},
   "source": [
    "### Initialize Hierarchical Course Manager\n",
    "\n",
    "The `HierarchicalCourseManager` provides two-tier retrieval:\n",
    "- **Tier 1:** Course summaries (lightweight, for search)\n",
    "- **Tier 2:** Full course details (on-demand)\n",
    "\n",
    "This is the same progressive disclosure pattern from Module 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5ec5b919d7cb17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:08.903333Z",
     "iopub.status.busy": "2025-12-09T19:56:08.903261Z",
     "iopub.status.idle": "2025-12-09T19:56:08.905029Z",
     "shell.execute_reply": "2025-12-09T19:56:08.904591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hierarchical Course Manager initialized\n",
      "   Two-tier retrieval: summaries ‚Üí details\n",
      "   Progressive disclosure pattern ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hierarchical Course Manager\n",
    "hierarchical_manager = HierarchicalCourseManager(redis_client=redis_client)\n",
    "context_assembler = HierarchicalContextAssembler()\n",
    "\n",
    "print(\"\"\"‚úÖ Hierarchical Course Manager initialized\n",
    "   Two-tier retrieval: summaries ‚Üí details\n",
    "   Progressive disclosure pattern ready\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a268c8a79e1b8",
   "metadata": {},
   "source": [
    "### Initialize LLM\n",
    "\n",
    "We'll use GPT-4o with temperature=0.0 for consistent, deterministic responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1191791c5443e47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:08.906148Z",
     "iopub.status.busy": "2025-12-09T19:56:08.906053Z",
     "iopub.status.idle": "2025-12-09T19:56:09.033314Z",
     "shell.execute_reply": "2025-12-09T19:56:09.032870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized (GPT-4o)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "print(\"‚úÖ LLM initialized (GPT-4o)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f22bb2fcb03f7",
   "metadata": {},
   "source": [
    "### Initialize Memory Client\n",
    "\n",
    "If the Agent Memory Server is available, we'll initialize the memory client. This client handles both working memory (conversation history) and long-term memory (persistent facts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59ce92b366fb180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.034867Z",
     "iopub.status.busy": "2025-12-09T19:56:09.034757Z",
     "iopub.status.idle": "2025-12-09T19:56:09.040599Z",
     "shell.execute_reply": "2025-12-09T19:56:09.040173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory Client initialized\n",
      "   Base URL: http://localhost:8088\n",
      "   Namespace: redis_university\n",
      "   Ready for working memory and long-term memory operations\n"
     ]
    }
   ],
   "source": [
    "# Initialize Memory Client\n",
    "if MEMORY_SERVER_AVAILABLE:\n",
    "    config = MemoryClientConfig(\n",
    "        base_url=AGENT_MEMORY_URL, default_namespace=\"redis_university\"\n",
    "    )\n",
    "    memory_client = MemoryAPIClient(config=config)\n",
    "    print(f\"\"\"‚úÖ Memory Client initialized\n",
    "   Base URL: {config.base_url}\n",
    "   Namespace: {config.default_namespace}\n",
    "   Ready for working memory and long-term memory operations\"\"\")\n",
    "else:\n",
    "    memory_client = None\n",
    "    print(\"\"\"‚ö†Ô∏è  Memory Server not available\n",
    "   Running with limited functionality\n",
    "   Some demos will be skipped\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2dbb1498e13fc",
   "metadata": {},
   "source": [
    "### Create Sample Student Profile\n",
    "\n",
    "We'll create a sample student profile to use throughout our demos. This follows the same pattern from Module 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe37f77d536494e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.041759Z",
     "iopub.status.busy": "2025-12-09T19:56:09.041685Z",
     "iopub.status.idle": "2025-12-09T19:56:09.045422Z",
     "shell.execute_reply": "2025-12-09T19:56:09.045104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Student profile created\n",
      "   Name: Sarah Chen\n",
      "   Major: Computer Science\n",
      "   Year: 2\n",
      "   Interests: machine learning, data science, algorithms\n",
      "   Completed: CS101, CS201\n",
      "   Preferred Format: online\n"
     ]
    }
   ],
   "source": [
    "# Create sample student profile\n",
    "sarah = StudentProfile(\n",
    "    name=\"Sarah Chen\",\n",
    "    email=\"sarah.chen@university.edu\",\n",
    "    major=\"Computer Science\",\n",
    "    year=2,\n",
    "    interests=[\"machine learning\", \"data science\", \"algorithms\"],\n",
    "    completed_courses=[\"CS101\", \"CS201\"],\n",
    "    current_courses=[\"MATH301\"],\n",
    "    preferred_format=CourseFormat.ONLINE,\n",
    "    preferred_difficulty=DifficultyLevel.INTERMEDIATE,\n",
    ")\n",
    "\n",
    "print(f\"\"\"‚úÖ Student profile created\n",
    "   Name: {sarah.name}\n",
    "   Major: {sarah.major}\n",
    "   Year: {sarah.year}\n",
    "   Interests: {', '.join(sarah.interests)}\n",
    "   Completed: {', '.join(sarah.completed_courses)}\n",
    "   Preferred Format: {sarah.preferred_format.value}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa912e1bd2e6235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.046610Z",
     "iopub.status.busy": "2025-12-09T19:56:09.046544Z",
     "iopub.status.idle": "2025-12-09T19:56:09.048422Z",
     "shell.execute_reply": "2025-12-09T19:56:09.047957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ INITIALIZATION SUMMARY\n",
      "\n",
      "‚úÖ Redis Connection: Ready\n",
      "‚úÖ Hierarchical Course Manager: Ready (two-tier retrieval)\n",
      "‚úÖ Context Assembler: Ready (progressive disclosure)\n",
      "‚úÖ LLM (GPT-4o): Ready\n",
      "‚úÖ Memory Client: Ready\n",
      "‚úÖ Student Profile: Sarah Chen\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"üéØ INITIALIZATION SUMMARY\n",
    "\n",
    "‚úÖ Redis Connection: Ready\n",
    "‚úÖ Hierarchical Course Manager: Ready (two-tier retrieval)\n",
    "‚úÖ Context Assembler: Ready (progressive disclosure)\n",
    "‚úÖ LLM (GPT-4o): Ready\n",
    "{'‚úÖ' if MEMORY_SERVER_AVAILABLE else '‚ö†Ô∏è '} Memory Client: {'Ready' if MEMORY_SERVER_AVAILABLE else 'Not Available'}\n",
    "‚úÖ Student Profile: {sarah.name}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59105c5db8d2eade",
   "metadata": {},
   "source": [
    "### Initialization Done\n",
    "\n",
    "üìã **What We're Building On:**\n",
    "- Module 2's RAG foundation (`HierarchicalCourseManager`, `redis_config`)\n",
    "- Same `StudentProfile` model\n",
    "- Same Redis configuration\n",
    "- Progressive disclosure pattern (summaries ‚Üí details)\n",
    "\n",
    "‚ú® **What We're Adding:**\n",
    "- Memory Client for conversation history\n",
    "- Working Memory for session context\n",
    "- Long-term Memory for persistent knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe569a41967dd57d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 1: Working Memory Fundamentals\n",
    "\n",
    "### **What is Working Memory?**\n",
    "\n",
    "Working memory stores **conversation messages** for the current session. It enables:\n",
    "\n",
    "- ‚úÖ **Reference resolution** - \"it\", \"that course\", \"the one you mentioned\"\n",
    "- ‚úÖ **Context continuity** - Each message builds on previous messages\n",
    "- ‚úÖ **Natural conversations** - Users don't repeat themselves\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "```\n",
    "Turn 1: Load working memory (empty) ‚Üí Process query ‚Üí Save messages\n",
    "Turn 2: Load working memory (1 exchange) ‚Üí Process query ‚Üí Save messages\n",
    "Turn 3: Load working memory (2 exchanges) ‚Üí Process query ‚Üí Save messages\n",
    "```\n",
    "\n",
    "Each turn has access to all previous messages in the session.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Hands-On: Working Memory in Action\n",
    "\n",
    "Let's simulate a multi-turn conversation with working memory. We'll break this down step-by-step to see how working memory enables natural conversation flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda6d53263e2372",
   "metadata": {},
   "source": [
    "### Setup: Create Session and Student IDs\n",
    "\n",
    "Now that we have our components initialized, let's create session and student identifiers for our working memory demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "473224677f6d5a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.049860Z",
     "iopub.status.busy": "2025-12-09T19:56:09.049749Z",
     "iopub.status.idle": "2025-12-09T19:56:09.051753Z",
     "shell.execute_reply": "2025-12-09T19:56:09.051350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Working Memory Demo Setup\n",
      "   Student ID: sarah.chen\n",
      "   Session ID: session_sarah.chen_demo\n",
      "   Ready to demonstrate multi-turn conversation\n"
     ]
    }
   ],
   "source": [
    "# Setup for working memory demo\n",
    "student_id = sarah.email.split(\"@\")[0]  # \"sarah.chen\"\n",
    "session_id = f\"session_{student_id}_demo\"\n",
    "\n",
    "print(f\"\"\"üéØ Working Memory Demo Setup\n",
    "   Student ID: {student_id}\n",
    "   Session ID: {session_id}\n",
    "   Ready to demonstrate multi-turn conversation\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833f8d7a24e241b",
   "metadata": {},
   "source": [
    "### Turn 1: Initial Query\n",
    "\n",
    "Let's start with a simple query about a course. This is the first turn, so working memory will be empty.\n",
    "\n",
    "We'll break this down into clear steps:\n",
    "1. Load working memory (will be empty on first turn)\n",
    "2. Search for courses using hierarchical retrieval\n",
    "3. Generate a response\n",
    "4. Save the conversation to working memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deed83ce5cd46fe",
   "metadata": {},
   "source": [
    "#### Step 1: Set up the user query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7eb329361c4dead",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.052849Z",
     "iopub.status.busy": "2025-12-09T19:56:09.052779Z",
     "iopub.status.idle": "2025-12-09T19:56:09.054784Z",
     "shell.execute_reply": "2025-12-09T19:56:09.054402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìç TURN 1: User asks about a course\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Tell me about machine learning courses\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìç TURN 1: User asks about a course\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define the user's query\n",
    "turn1_query = \"Tell me about machine learning courses\"\n",
    "print(f\"\\nüë§ User: {turn1_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7f40c8923de74",
   "metadata": {},
   "source": [
    "#### Step 2: Load working memory\n",
    "\n",
    "On the first turn, working memory will be empty since this is a new session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d6fbb76fd35682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.055880Z",
     "iopub.status.busy": "2025-12-09T19:56:09.055814Z",
     "iopub.status.idle": "2025-12-09T19:56:09.071803Z",
     "shell.execute_reply": "2025-12-09T19:56:09.071279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    üìä Working Memory Status:\n",
      "    Messages in memory: 24\n",
      "    Status: Has history\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load working memory (empty for first turn)\n",
    "_, turn1_working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "    üìä Working Memory Status:\n",
    "    Messages in memory: {len(turn1_working_memory.messages)}\n",
    "    Status: {'Empty (first turn)' if len(turn1_working_memory.messages) == 0 else 'Has history'}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cdf56f6d3481d",
   "metadata": {},
   "source": [
    "#### Step 3: Search for courses using hierarchical retrieval\n",
    "\n",
    "Use the hierarchical manager to search for courses. This uses the progressive disclosure pattern:\n",
    "- First, get summaries (lightweight)\n",
    "- Then, fetch details for top results (on-demand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b5f90d38dc581b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.073598Z",
     "iopub.status.busy": "2025-12-09T19:56:09.073498Z",
     "iopub.status.idle": "2025-12-09T19:56:09.306737Z",
     "shell.execute_reply": "2025-12-09T19:56:09.306114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for courses using hierarchical retrieval...\n",
      "14:56:09 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'Tell me about machine learning courses' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 redis_context_course.hierarchical_manager INFO   Created summary index: course_summaries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: Tell me about machine learning courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 redis_context_course.hierarchical_manager INFO   Fetched 2 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:09 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 3 summaries, 2 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 3 summaries, fetched 2 details\n",
      "   Progressive disclosure: summaries first, details on-demand\n",
      "\n",
      "   üìã Course Summaries:\n",
      "      1. CS012: Machine Learning Fundamentals\n",
      "      2. MATH022: Linear Algebra for Machine Learning\n",
      "      3. CS013: Deep Learning and Neural Networks\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Searching for courses using hierarchical retrieval...\")\n",
    "\n",
    "# Use hierarchical search (summaries + details)\n",
    "turn1_summaries, turn1_details = await hierarchical_manager.hierarchical_search(\n",
    "    query=turn1_query,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "print(f\"\"\"   Found {len(turn1_summaries)} summaries, fetched {len(turn1_details)} details\n",
    "   Progressive disclosure: summaries first, details on-demand\"\"\")\n",
    "\n",
    "# Show what we found\n",
    "if turn1_summaries:\n",
    "    print(\"\\n   üìã Course Summaries:\")\n",
    "    for i, summary in enumerate(turn1_summaries[:3], 1):\n",
    "        print(f\"      {i}. {summary.course_code}: {summary.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd73a8a3051d5ac",
   "metadata": {},
   "source": [
    "#### Step 4: Assemble context and generate response\n",
    "\n",
    "Use the context assembler to build context with progressive disclosure, then generate a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce7ff089f5f62b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:09.308322Z",
     "iopub.status.busy": "2025-12-09T19:56:09.308204Z",
     "iopub.status.idle": "2025-12-09T19:56:13.456106Z",
     "shell.execute_reply": "2025-12-09T19:56:13.455361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìù Context assembled (9514 characters)\n",
      "\n",
      "üí≠ Generating response using LLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:13 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Here are three machine learning-related courses you might be interested in:\n",
      "\n",
      "1. **CS012: Machine Learning Fundamentals**\n",
      "   - **Department**: Computer Science\n",
      "   - **Instructor**: Jamie Arnold\n",
      "   - **Credits**: 4 | **Level**: Advanced\n",
      "   - **Format**: In-person\n",
      "   - **Description**: This course introduces machine learning algorithms and applications, covering supervised and unsupervised learning, neural networks, and deep learning. Students will implement algorithms from scratch and use frameworks like scikit-learn and TensorFlow.\n",
      "   - **Prerequisites**: CS002, MATH020\n",
      "\n",
      "2. **MATH022: Linear Algebra for Machine Learning**\n",
      "   - **Department**: Mathematics\n",
      "   - **Instructor**: Patty Perez\n",
      "   - **Credits**: 4 | **Level**: Intermediate\n",
      "   - **Format**: In-person\n",
      "   - **Description**: This course provides the mathematical foundation for machine learning, focusing on vectors, matrices, and linear transformations. Topics include vector spaces, eigenvalues, SVD, and their applications in ML.\n",
      "   - **Tags**: Linear algebra, mathematics, machine learning\n",
      "\n",
      "3. **CS013: Deep Learning and Neural Networks**\n",
      "   - **Department**: Computer Science\n",
      "   - **Instructor**: Lisa Hensley\n",
      "   - **Credits**: 4 | **Level**: Advanced\n",
      "   - **Format**: Online\n",
      "   - **Description**: This course covers advanced neural network architectures and deep learning techniques.\n",
      "   - **Prerequisites**: CS010\n",
      "\n",
      "Each course offers a unique perspective on machine learning, from foundational algorithms to the mathematical underpinnings and advanced deep learning techniques.\n"
     ]
    }
   ],
   "source": [
    "# Assemble context using progressive disclosure\n",
    "turn1_context = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=turn1_summaries,\n",
    "    details=turn1_details,\n",
    "    query=turn1_query\n",
    ")\n",
    "\n",
    "print(f\"   üìù Context assembled ({len(turn1_context)} characters)\")\n",
    "\n",
    "# Build messages for LLM\n",
    "turn1_messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful course advisor. Answer questions about courses based on the provided information. Be concise but informative.\"\n",
    "    ),\n",
    "    HumanMessage(content=f\"{turn1_context}\\n\\nUser question: {turn1_query}\"),\n",
    "]\n",
    "\n",
    "# Generate response using LLM\n",
    "print(\"\\nüí≠ Generating response using LLM...\")\n",
    "turn1_response = llm.invoke(turn1_messages).content\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {turn1_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ed0939f36c7a7",
   "metadata": {},
   "source": [
    "#### Step 5: Save to working memory\n",
    "\n",
    "Add both the user query and assistant response to working memory for future turns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2fb37911816d8c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:13.458395Z",
     "iopub.status.busy": "2025-12-09T19:56:13.458218Z",
     "iopub.status.idle": "2025-12-09T19:56:13.479261Z",
     "shell.execute_reply": "2025-12-09T19:56:13.478308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:13 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Saved to working memory\n",
      "    Messages now in memory: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add messages to working memory\n",
    "turn1_working_memory.messages.extend(\n",
    "    [\n",
    "        MemoryMessage(role=\"user\", content=turn1_query),\n",
    "        MemoryMessage(role=\"assistant\", content=turn1_response),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save to Memory Server\n",
    "await memory_client.put_working_memory(\n",
    "    session_id=session_id,\n",
    "    memory=turn1_working_memory,\n",
    "    user_id=student_id,\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "    Saved to working memory\n",
    "    Messages now in memory: {len(turn1_working_memory.messages)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42840aa50ad80557",
   "metadata": {},
   "source": [
    "### What Just Happened in Turn 1?\n",
    "\n",
    "**Initial State:**\n",
    "- Working memory was empty (first turn)\n",
    "- No conversation history available\n",
    "\n",
    "**Actions (RAG Pattern with Progressive Disclosure):**\n",
    "1. **Retrieve:** Used hierarchical search (summaries ‚Üí details)\n",
    "2. **Augment:** Assembled context with progressive disclosure\n",
    "3. **Generate:** LLM created a natural language response\n",
    "4. **Save:** Stored conversation in working memory\n",
    "\n",
    "**Result:**\n",
    "- Working memory now contains 2 messages (1 user, 1 assistant)\n",
    "- This history will be available for the next turn\n",
    "\n",
    "**Key Insight:** We used the same hierarchical retrieval pattern from Module 2, now combined with memory!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a231ce0fc6a748a",
   "metadata": {},
   "source": [
    "### Turn 2: Follow-up with Pronoun Reference\n",
    "\n",
    "Now let's ask a follow-up question using \"it\" - a pronoun that requires context from Turn 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5629b2461afd0da",
   "metadata": {},
   "source": [
    "#### Step 1: Set up the query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c58976432bee9a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:13.481326Z",
     "iopub.status.busy": "2025-12-09T19:56:13.481181Z",
     "iopub.status.idle": "2025-12-09T19:56:13.484008Z",
     "shell.execute_reply": "2025-12-09T19:56:13.483362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 2: User uses pronoun reference ('it')\n",
      "================================================================================\n",
      "\n",
      "üë§ User: What are the prerequisites for it?\n",
      "   Note: 'it' refers to a course from Turn 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 2: User uses pronoun reference ('it')\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "turn2_query = \"What are the prerequisites for it?\"\n",
    "print(f\"\\nüë§ User: {turn2_query}\")\n",
    "print(\"   Note: 'it' refers to a course from Turn 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3007669c59ae5",
   "metadata": {},
   "source": [
    "#### Step 2: Load working memory\n",
    "\n",
    "This time, working memory will contain the conversation from Turn 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36310565dd00c237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:13.485586Z",
     "iopub.status.busy": "2025-12-09T19:56:13.485456Z",
     "iopub.status.idle": "2025-12-09T19:56:13.495375Z",
     "shell.execute_reply": "2025-12-09T19:56:13.494815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:13 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Working Memory Status:\n",
      "   Messages in memory: 26\n",
      "   Contains: Turn 1 conversation\n"
     ]
    }
   ],
   "source": [
    "# Load working memory (now has 1 exchange from Turn 1)\n",
    "_, turn2_working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Working Memory Status:\n",
    "   Messages in memory: {len(turn2_working_memory.messages)}\n",
    "   Contains: Turn 1 conversation\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835a58cba2bf1b6",
   "metadata": {},
   "source": [
    "#### Step 3: Build context with conversation history\n",
    "\n",
    "To resolve the pronoun \"it\", we need to include the conversation history in the LLM context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cab346e89ea60dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:13.496856Z",
     "iopub.status.busy": "2025-12-09T19:56:13.496750Z",
     "iopub.status.idle": "2025-12-09T19:56:13.499707Z",
     "shell.execute_reply": "2025-12-09T19:56:13.499338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Building context with conversation history...\n",
      "   Total messages in context: 28\n",
      "Includes: System prompt + Turn 1 history + current query\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîß Building context with conversation history...\")\n",
    "\n",
    "# Start with system message\n",
    "turn2_messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful course advisor. Use conversation history to resolve references like 'it', 'that course', etc. Be concise but informative.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add conversation history from working memory\n",
    "for msg in turn2_working_memory.messages:\n",
    "    if msg.role == \"user\":\n",
    "        turn2_messages.append(HumanMessage(content=msg.content))\n",
    "    elif msg.role == \"assistant\":\n",
    "        turn2_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "# Add current query\n",
    "turn2_messages.append(HumanMessage(content=turn2_query))\n",
    "\n",
    "print(f\"\"\"   Total messages in context: {len(turn2_messages)}\n",
    "Includes: System prompt + Turn 1 history + current query\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af331a75d883304",
   "metadata": {},
   "source": [
    "#### Step 4: Generate response using LLM\n",
    "\n",
    "The LLM can now resolve \"it\" by looking at the conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98780f327a7c485f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:13.501305Z",
     "iopub.status.busy": "2025-12-09T19:56:13.501195Z",
     "iopub.status.idle": "2025-12-09T19:56:15.440895Z",
     "shell.execute_reply": "2025-12-09T19:56:15.440014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí≠ LLM resolving 'it' using conversation history...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:15 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: The prerequisites for the machine learning courses mentioned are as follows:\n",
      "\n",
      "1. **CS012: Machine Learning Fundamentals**\n",
      "   - **Prerequisites**: CS002 (an introductory computer science course) and MATH020 (a foundational mathematics course).\n",
      "\n",
      "2. **MATH022: Linear Algebra for Machine Learning**\n",
      "   - **Prerequisites**: While specific prerequisites aren't listed, a basic understanding of algebra and calculus is typically expected.\n",
      "\n",
      "3. **CS013: Deep Learning and Neural Networks**\n",
      "   - **Prerequisites**: CS010 (a course that likely covers introductory machine learning concepts).\n",
      "\n",
      "These prerequisites ensure that students have the necessary background in computer science and mathematics to effectively engage with the course material.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüí≠ LLM resolving 'it' using conversation history...\")\n",
    "turn2_response = llm.invoke(turn2_messages).content\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {turn2_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e66854646ae9f",
   "metadata": {},
   "source": [
    "#### Step 5: Save to working memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73603a25026b0a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:15.443192Z",
     "iopub.status.busy": "2025-12-09T19:56:15.443005Z",
     "iopub.status.idle": "2025-12-09T19:56:15.458789Z",
     "shell.execute_reply": "2025-12-09T19:56:15.457950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:15 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved to working memory\n",
      "   Messages now in memory: 28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add messages to working memory\n",
    "turn2_working_memory.messages.extend(\n",
    "    [\n",
    "        MemoryMessage(role=\"user\", content=turn2_query),\n",
    "        MemoryMessage(role=\"assistant\", content=turn2_response),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save to Memory Server\n",
    "await memory_client.put_working_memory(\n",
    "    session_id=session_id,\n",
    "    memory=turn2_working_memory,\n",
    "    user_id=student_id,\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Saved to working memory\n",
    "   Messages now in memory: {len(turn2_working_memory.messages)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c399e940b2ac",
   "metadata": {},
   "source": [
    "### What Just Happened in Turn 2?\n",
    "\n",
    "**Initial State:**\n",
    "- Working memory contained Turn 1 conversation (2 messages)\n",
    "- User asked about \"its prerequisites\" - pronoun reference\n",
    "\n",
    "**Actions:**\n",
    "1. Loaded working memory with Turn 1 history\n",
    "2. Built context including conversation history\n",
    "3. LLM resolved \"it\" ‚Üí the course from Turn 1\n",
    "4. Generated response about prerequisites\n",
    "5. Saved updated conversation to working memory\n",
    "\n",
    "**Result:**\n",
    "- Working memory now contains 4 messages (2 exchanges)\n",
    "- LLM successfully resolved pronoun reference using conversation history\n",
    "- Natural conversation flow maintained\n",
    "\n",
    "**Key Insight:** Without working memory, the LLM wouldn't know what \"it\" refers to!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7073faa996dab1",
   "metadata": {},
   "source": [
    "### Turn 3: Another Follow-up\n",
    "\n",
    "Let's ask one more follow-up question to demonstrate continued conversation continuity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c127596e4cffd40d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:15.460579Z",
     "iopub.status.busy": "2025-12-09T19:56:15.460457Z",
     "iopub.status.idle": "2025-12-09T19:56:17.657728Z",
     "shell.execute_reply": "2025-12-09T19:56:17.657188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 3: User asks another follow-up\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Is it available online?\n",
      "   Note: 'it' still refers to the course from Turn 1\n",
      "14:56:15 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Working Memory Status:\n",
      "Messages in memory: 28\n",
      "Contains: Turns 1 and 2\n",
      "   Total messages in context: 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:17 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Yes, one of the courses mentioned is available online:\n",
      "\n",
      "- **CS013: Deep Learning and Neural Networks** is offered in an online format. This course covers advanced neural network architectures and deep learning techniques, making it accessible for remote learning.\n",
      "\n",
      "The other courses, **CS012: Machine Learning Fundamentals** and **MATH022: Linear Algebra for Machine Learning**, are offered in-person. If you're specifically looking for online options, you might also consider exploring platforms like Coursera, edX, or Udacity, which offer a wide range of online machine learning courses.\n",
      "14:56:17 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/session_sarah.chen_demo?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved to working memory\n",
      "   Messages now in memory: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 3: User asks another follow-up\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "turn3_query = \"Is it available online?\"\n",
    "print(f\"\\nüë§ User: {turn3_query}\")\n",
    "print(\"   Note: 'it' still refers to the course from Turn 1\")\n",
    "\n",
    "# Load working memory (now has 2 exchanges)\n",
    "_, turn3_working_memory = await memory_client.get_or_create_working_memory(\n",
    "    session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Working Memory Status:\n",
    "Messages in memory: {len(turn3_working_memory.messages)}\n",
    "Contains: Turns 1 and 2\"\"\")\n",
    "\n",
    "# Build context with full conversation history\n",
    "turn3_messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful course advisor. Use conversation history to resolve references.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for msg in turn3_working_memory.messages:\n",
    "    if msg.role == \"user\":\n",
    "        turn3_messages.append(HumanMessage(content=msg.content))\n",
    "    elif msg.role == \"assistant\":\n",
    "        turn3_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "turn3_messages.append(HumanMessage(content=turn3_query))\n",
    "\n",
    "print(f\"   Total messages in context: {len(turn3_messages)}\")\n",
    "\n",
    "# Generate response\n",
    "turn3_response = llm.invoke(turn3_messages).content\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {turn3_response}\")\n",
    "\n",
    "# Save to working memory\n",
    "turn3_working_memory.messages.extend(\n",
    "    [\n",
    "        MemoryMessage(role=\"user\", content=turn3_query),\n",
    "        MemoryMessage(role=\"assistant\", content=turn3_response),\n",
    "    ]\n",
    ")\n",
    "\n",
    "await memory_client.put_working_memory(\n",
    "    session_id=session_id,\n",
    "    memory=turn3_working_memory,\n",
    "    user_id=student_id,\n",
    "    model_name=\"gpt-4o\",\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Saved to working memory\n",
    "   Messages now in memory: {len(turn3_working_memory.messages)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37c05d024d6df5",
   "metadata": {},
   "source": [
    "### üéØ Working Memory Demo Summary\n",
    "\n",
    "**üìä What Happened:**\n",
    "\n",
    "| Turn | Query | Working Memory | Result |\n",
    "|------|-------|----------------|--------|\n",
    "| 1 | \"Tell me about machine learning courses\" | Empty (first turn) | Stored query + response |\n",
    "| 2 | \"What are the prerequisites for it?\" | 1 exchange | LLM resolved 'it' using history |\n",
    "| 3 | \"Is it available online?\" | 2 exchanges | Continued conversation flow |\n",
    "\n",
    "**‚úÖ Key Benefits:**\n",
    "- Natural conversation flow\n",
    "- Pronoun reference resolution\n",
    "- No need to repeat context\n",
    "- Seamless user experience\n",
    "\n",
    "**‚ùå Without Working Memory:**\n",
    "- \"What are the prerequisites for it?\" ‚Üí \"What is 'it'? Please specify.\"\n",
    "- Each query is isolated\n",
    "- User must repeat context every time\n",
    "\n",
    "### Key Insight: Conversation Context Type\n",
    "\n",
    "Working memory provides the **Conversation Context** - the third context type from Module 1:\n",
    "\n",
    "1. **System Context** - Role and instructions (static)\n",
    "2. **User Context** - Profile and preferences (dynamic, user-specific)\n",
    "3. **Conversation Context** - Working memory (dynamic, session-specific) ‚Üê **We just demonstrated this!**\n",
    "4. **Retrieved Context** - RAG results (dynamic, query-specific)\n",
    "\n",
    "Without working memory, we only had 3 context types. Now we have all 4!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cc39a22fde44a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 2: Long-term Memory for Context Engineering\n",
    "\n",
    "### What is Long-term Memory?\n",
    "\n",
    "Long-term memory enables AI agents to store **persistent knowledge** across sessions‚Äîincluding user preferences, domain facts, business rules, and system configuration. This is crucial for context engineering because it allows agents to:\n",
    "\n",
    "- **Personalize** interactions by remembering user-specific preferences and history\n",
    "- **Apply domain knowledge** consistently (prerequisites, policies, regulations)\n",
    "- **Maintain organizational context** (business rules, schedules, procedures)\n",
    "- **Search efficiently** using semantic vector search across all knowledge types\n",
    "\n",
    "Long-term memory is a flexible storage mechanism: user-scoped memories enable personalization (\"Student prefers online courses\"), while application-scoped memories provide consistent behavior for everyone (\"CS401 requires CS201\", \"Registration opens 2 weeks before semester\").\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Session 1: User shares preferences ‚Üí Store in long-term memory\n",
    "Session 2: User asks for recommendations ‚Üí Search memory ‚Üí Personalized response\n",
    "Session 3: User updates preferences ‚Üí Update memory accordingly\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Three Types of Long-term Memory\n",
    "\n",
    "The Agent Memory Server supports three distinct memory types, each optimized for different kinds of information:\n",
    "\n",
    "### 1. Semantic Memory - Facts and Knowledge\n",
    "\n",
    "**Purpose:** Store timeless facts, preferences, and knowledge independent of when they were learned. Can be user-scoped (personalization) or application-scoped (domain knowledge).\n",
    "\n",
    "**User-Scoped Examples:**\n",
    "- \"Student's major is Computer Science\"\n",
    "- \"Student prefers online courses\"\n",
    "- \"Student wants to graduate in Spring 2026\"\n",
    "\n",
    "**Application-Scoped Examples:**\n",
    "- \"CS401 requires CS201 and MATH301 as prerequisites\"\n",
    "- \"Online courses have asynchronous discussion forums\"\n",
    "- \"Maximum file upload size for assignments is 50MB\"\n",
    "\n",
    "**When to use:** Information that remains true regardless of time context.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Episodic Memory - Events and Experiences\n",
    "\n",
    "**Purpose:** Store time-bound events and experiences where sequence matters.\n",
    "\n",
    "**Examples:**\n",
    "- \"Student enrolled in CS101 on 2024-09-15\"\n",
    "- \"Student completed CS101 with grade A on 2024-12-10\"\n",
    "- \"Student asked about machine learning courses on 2024-09-20\"\n",
    "\n",
    "**When to use:** Timeline-based information where timing or sequence is important.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Message Memory - Context-Rich Conversations\n",
    "\n",
    "**Purpose:** Store full conversation snippets where complete context is crucial.\n",
    "\n",
    "**Examples:**\n",
    "- Detailed career planning discussion with nuanced advice\n",
    "- Professor's specific guidance about research opportunities\n",
    "- Student's explanation of personal learning challenges\n",
    "\n",
    "**When to use:** When summary would lose important nuance, tone, or exact wording.\n",
    "\n",
    "**‚ö†Ô∏è Use sparingly** - Message memories are token-expensive!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Choosing the Right Memory Type\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "**Ask yourself these questions:**\n",
    "\n",
    "1. **Can you extract a simple fact?** ‚Üí Use **Semantic**\n",
    "2. **Does timing matter?** ‚Üí Use **Episodic**\n",
    "3. **Is full context crucial?** ‚Üí Use **Message** (rarely)\n",
    "\n",
    "**Default strategy: Prefer Semantic** - they're compact, searchable, and efficient.\n",
    "\n",
    "### Quick Reference Table\n",
    "\n",
    "| Information Type | Memory Type | Example |\n",
    "|-----------------|-------------|----------|\n",
    "| Preference | Semantic | \"Prefers morning classes\" |\n",
    "| Fact | Semantic | \"Major is Computer Science\" |\n",
    "| Goal | Semantic | \"Wants to graduate in 2026\" |\n",
    "| Event | Episodic | \"Enrolled in CS401 on 2024-09-15\" |\n",
    "| Timeline | Episodic | \"Completed CS101, then CS201\" |\n",
    "| Complex discussion | Message | [Full career planning conversation] |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Hands-On: Long-term Memory in Action\n",
    "\n",
    "Let's put these concepts into practice with code examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5117be1ffcf32e9",
   "metadata": {},
   "source": [
    "### Setup: Student ID for Long-term Memory\n",
    "\n",
    "Long-term memories are user-scoped, so we need a student ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf6440b84281516b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:17.659210Z",
     "iopub.status.busy": "2025-12-09T19:56:17.659114Z",
     "iopub.status.idle": "2025-12-09T19:56:17.661185Z",
     "shell.execute_reply": "2025-12-09T19:56:17.660712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Long-term Memory Demo Setup\n",
      "   Student ID: sarah_chen\n",
      "   Ready to store and search persistent memories\n"
     ]
    }
   ],
   "source": [
    "# Setup for long-term memory demo\n",
    "lt_student_id = \"sarah_chen\"\n",
    "\n",
    "print(f\"\"\"üéØ Long-term Memory Demo Setup\n",
    "   Student ID: {lt_student_id}\n",
    "   Ready to store and search persistent memories\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3571daa56c7415a",
   "metadata": {},
   "source": [
    "### Step 1: Store Semantic Memories (Facts)\n",
    "\n",
    "Semantic memories are timeless facts about the student. Let's store several facts about Sarah's preferences and academic status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10adf1e40476f738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:17.662592Z",
     "iopub.status.busy": "2025-12-09T19:56:17.662494Z",
     "iopub.status.idle": "2025-12-09T19:56:17.680312Z",
     "shell.execute_reply": "2025-12-09T19:56:17.679716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìç STEP 1: Storing Semantic Memories (Facts)\n",
      "================================================================================\n",
      "\n",
      "üìù Storing 6 semantic memories...\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers online courses over in-person classes\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student's major is Computer Science with focus on AI/ML\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student wants to graduate in Spring 2026\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers morning classes, no classes on Fridays\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student has completed Introduction to Programming and Data Structures\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student is currently taking Linear Algebra\n",
      "\n",
      "‚úÖ Stored 6 semantic memories\n",
      "   Memory type: semantic (timeless facts)\n",
      "   Topics: preferences, academic_info\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìç STEP 1: Storing Semantic Memories (Facts)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define semantic memories (timeless facts)\n",
    "semantic_memories = [\n",
    "    \"Student prefers online courses over in-person classes\",\n",
    "    \"Student's major is Computer Science with focus on AI/ML\",\n",
    "    \"Student wants to graduate in Spring 2026\",\n",
    "    \"Student prefers morning classes, no classes on Fridays\",\n",
    "    \"Student has completed Introduction to Programming and Data Structures\",\n",
    "    \"Student is currently taking Linear Algebra\",\n",
    "]\n",
    "print(f\"\\nüìù Storing {len(semantic_memories)} semantic memories...\")\n",
    "\n",
    "# Store each semantic memory\n",
    "for memory_text in semantic_memories:\n",
    "    memory_record = ClientMemoryRecord(\n",
    "        text=memory_text,\n",
    "        user_id=lt_student_id,\n",
    "        memory_type=\"semantic\",\n",
    "        topics=[\"preferences\", \"academic_info\"],\n",
    "    )\n",
    "    await memory_client.create_long_term_memory([memory_record])\n",
    "    print(f\"   ‚úÖ {memory_text}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Stored {len(semantic_memories)} semantic memories\n",
    "   Memory type: semantic (timeless facts)\n",
    "   Topics: preferences, academic_info\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e7139023e0182",
   "metadata": {},
   "source": [
    "### What We Just Did: Semantic Memories\n",
    "\n",
    "**Stored 6 semantic memories:**\n",
    "- Student preferences (online courses, morning classes)\n",
    "- Academic information (major, graduation date)\n",
    "- Course history (completed, current)\n",
    "\n",
    "**Why semantic?**\n",
    "- These are timeless facts\n",
    "- No specific date/time context needed\n",
    "- Compact and efficient\n",
    "\n",
    "**How they're stored:**\n",
    "- Vector-indexed for semantic search\n",
    "- Tagged with topics for organization\n",
    "- Automatically deduplicated\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d11fddf0f3e95",
   "metadata": {},
   "source": [
    "### Step 2: Store Episodic Memories (Events)\n",
    "\n",
    "Episodic memories are time-bound events. Let's store some events from Sarah's academic timeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9eaf03f0ea2c003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:17.682190Z",
     "iopub.status.busy": "2025-12-09T19:56:17.682049Z",
     "iopub.status.idle": "2025-12-09T19:56:17.690628Z",
     "shell.execute_reply": "2025-12-09T19:56:17.690040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç STEP 2: Storing Episodic Memories (Events)\n",
      "================================================================================\n",
      "\n",
      "üìù Storing 3 episodic memories...\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student enrolled in Introduction to Programming on 2024-09-01\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student completed Introduction to Programming with grade A on 2024-12-15\n",
      "14:56:17 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student asked about machine learning courses on 2024-09-20\n",
      "\n",
      "‚úÖ Stored 3 episodic memories\n",
      "   Memory type: episodic (time-bound events)\n",
      "   Topics: enrollment, courses\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç STEP 2: Storing Episodic Memories (Events)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define episodic memories (time-bound events)\n",
    "episodic_memories = [\n",
    "    \"Student enrolled in Introduction to Programming on 2024-09-01\",\n",
    "    \"Student completed Introduction to Programming with grade A on 2024-12-15\",\n",
    "    \"Student asked about machine learning courses on 2024-09-20\",\n",
    "]\n",
    "\n",
    "print(f\"\\nüìù Storing {len(episodic_memories)} episodic memories...\")\n",
    "\n",
    "# Store each episodic memory\n",
    "for memory_text in episodic_memories:\n",
    "    memory_record = ClientMemoryRecord(\n",
    "        text=memory_text,\n",
    "        user_id=lt_student_id,\n",
    "        memory_type=\"episodic\",\n",
    "        topics=[\"enrollment\", \"courses\"],\n",
    "    )\n",
    "    await memory_client.create_long_term_memory([memory_record])\n",
    "    print(f\"   ‚úÖ {memory_text}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Stored {len(episodic_memories)} episodic memories\n",
    "   Memory type: episodic (time-bound events)\n",
    "   Topics: enrollment, courses\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76039eb5e847a017",
   "metadata": {},
   "source": [
    "### What We Just Did: Episodic Memories\n",
    "\n",
    "**Stored 3 episodic memories:**\n",
    "- Enrollment event (Introduction to Programming on 2024-09-01)\n",
    "- Completion event (Introduction to Programming with grade A on 2024-12-15)\n",
    "- Interaction event (asked about ML courses on 2024-09-20)\n",
    "\n",
    "**Why episodic?**\n",
    "- These are time-bound events\n",
    "- Timing and sequence matter\n",
    "- Captures academic timeline\n",
    "\n",
    "**Difference from semantic:**\n",
    "- Semantic: \"Student has completed Introduction to Programming\" (timeless fact)\n",
    "- Episodic: \"Student completed Introduction to Programming with grade A on 2024-12-15\" (specific event)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185cb8ea27b023a",
   "metadata": {},
   "source": [
    "### Step 3: Search Long-term Memory\n",
    "\n",
    "Now let's search our long-term memories using natural language queries. The system will use semantic search to find relevant memories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56fc1a610c400036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:17.691781Z",
     "iopub.status.busy": "2025-12-09T19:56:17.691697Z",
     "iopub.status.idle": "2025-12-09T19:56:18.955476Z",
     "shell.execute_reply": "2025-12-09T19:56:18.954550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç STEP 3: Searching Long-term Memory\n",
      "================================================================================\n",
      "\n",
      "üîç Query: 'What does the student prefer?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìö Found 3 relevant memories:\n",
      "      1. The student prefers online courses over in-person classes.\n",
      "      2. Student prefers morning classes\n",
      "      3. Student prefers morning classes, no classes on Fridays\n",
      "\n",
      "üîç Query: 'What courses has the student completed?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìö Found 5 relevant memories:\n",
      "      1. Student has completed Introduction to Programming and Data Structures\n",
      "      2. Student completed Introduction to Programming with grade A on 2024-12-15\n",
      "      3. Student's major is Computer Science\n",
      "      4. Student is currently taking Linear Algebra\n",
      "      5. Student asked about machine learning courses on 2024-09-20\n",
      "\n",
      "üîç Query: 'What is the student's major?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìö Found 3 relevant memories:\n",
      "      1. Student's major is Computer Science\n",
      "      2. Student's major is Computer Science with focus on AI/ML\n",
      "      3. Student wants to graduate in Spring 2026\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DEMO COMPLETE: Long-term memory enables persistent knowledge!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from agent_memory_client.filters import UserId\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç STEP 3: Searching Long-term Memory\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Query 1: What does the student prefer?\n",
    "search_query_1 = \"What does the student prefer?\"\n",
    "print(f\"\\nüîç Query: '{search_query_1}'\")\n",
    "\n",
    "search_results_1 = await memory_client.search_long_term_memory(\n",
    "    text=search_query_1, user_id=UserId(eq=lt_student_id), limit=3\n",
    ")\n",
    "\n",
    "if search_results_1.memories:\n",
    "    print(f\"   üìö Found {len(search_results_1.memories)} relevant memories:\")\n",
    "    for i, memory in enumerate(search_results_1.memories[:3], 1):\n",
    "        print(f\"      {i}. {memory.text}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "# Query 2: What courses has the student completed?\n",
    "search_query_2 = \"What courses has the student completed?\"\n",
    "print(f\"\\nüîç Query: '{search_query_2}'\")\n",
    "\n",
    "search_results_2 = await memory_client.search_long_term_memory(\n",
    "    text=search_query_2, user_id=UserId(eq=lt_student_id), limit=5\n",
    ")\n",
    "\n",
    "if search_results_2.memories:\n",
    "    print(f\"   üìö Found {len(search_results_2.memories)} relevant memories:\")\n",
    "    for i, memory in enumerate(search_results_2.memories[:5], 1):\n",
    "        print(f\"      {i}. {memory.text}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "# Query 3: What is the student's major?\n",
    "search_query_3 = \"What is the student's major?\"\n",
    "print(f\"\\nüîç Query: '{search_query_3}'\")\n",
    "\n",
    "search_results_3 = await memory_client.search_long_term_memory(\n",
    "    text=search_query_3, user_id=UserId(eq=lt_student_id), limit=3\n",
    ")\n",
    "\n",
    "if search_results_3.memories:\n",
    "    print(f\"   üìö Found {len(search_results_3.memories)} relevant memories:\")\n",
    "    for i, memory in enumerate(search_results_3.memories[:3], 1):\n",
    "        print(f\"      {i}. {memory.text}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DEMO COMPLETE: Long-term memory enables persistent knowledge!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf894047127804ac",
   "metadata": {},
   "source": [
    "### üéØ Long-term Memory Demo Summary\n",
    "\n",
    "**üìä What We Did:**\n",
    "- **Step 1:** Stored 6 semantic memories (facts) - preferences, major, graduation date\n",
    "- **Step 2:** Stored 3 episodic memories (events) - enrollment, completion, interaction\n",
    "- **Step 3:** Searched long-term memory with natural language queries\n",
    "\n",
    "**‚úÖ Key Benefits:**\n",
    "- Persistent knowledge across sessions\n",
    "- Semantic search (not keyword matching)\n",
    "- Automatic deduplication\n",
    "- Topic-based organization\n",
    "\n",
    "**üí° Key Insight:**\n",
    "Long-term memory enables personalization and knowledge accumulation across sessions. It's the foundation for building agents that remember and learn from users.\n",
    "\n",
    "### Key Insight: User Context Type\n",
    "\n",
    "Long-term memory provides part of the **User Context** - the second context type from Module 1:\n",
    "\n",
    "1. **System Context** - Role and instructions (static)\n",
    "2. **User Context** - Profile + long-term memories (dynamic, user-specific) ‚Üê **Long-term memories contribute here!**\n",
    "3. **Conversation Context** - Working memory (dynamic, session-specific)\n",
    "4. **Retrieved Context** - RAG results (dynamic, query-specific)\n",
    "\n",
    "Long-term memories enhance User Context by adding persistent knowledge about the user's preferences, history, and goals.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Understanding Memory Search: Why Semantic Only?\n",
    "\n",
    "You might have noticed that the Agent Memory Server uses **semantic (vector) search only** - no keyword search or hybrid search. Let's understand why this is the right choice.\n",
    "\n",
    "### Memory vs. Course Catalog: Different Search Needs\n",
    "\n",
    "| Aspect | Memory Search | Course Catalog Search |\n",
    "|--------|---------------|----------------------|\n",
    "| **Data Type** | Conversational facts | Structured catalog |\n",
    "| **Content** | \"Student prefers online courses\"<br>\"Completed CS101 last semester\" | Course codes, titles, syllabi<br>Departments, prerequisites |\n",
    "| **Queries** | \"What does the student prefer?\"<br>\"What courses has the student taken?\" | \"CS101\"<br>\"beginner programming courses\" |\n",
    "| **Exact Matches?** | ‚ùå No codes/IDs to match | ‚úÖ Course codes, departments |\n",
    "| **Best Search** | **Semantic only** | **Hybrid (semantic + keyword)** |\n",
    "\n",
    "### Why Semantic Search for Memories?\n",
    "\n",
    "**1. Conversational Content**\n",
    "```python\n",
    "# Memory content is natural language\n",
    "memories = [\n",
    "    \"Student prefers online courses over in-person\",\n",
    "    \"Interested in machine learning and AI\",\n",
    "    \"Completed CS101 with grade A last semester\"\n",
    "]\n",
    "\n",
    "# Queries are also natural language\n",
    "query = \"What does the student prefer?\"\n",
    "# ‚úÖ Semantic search finds: \"Student prefers online courses...\"\n",
    "# ‚ùå Keyword search would miss it (no exact word \"prefer\" in memory)\n",
    "```\n",
    "\n",
    "**2. No Exact Codes/IDs**\n",
    "```python\n",
    "# Memories don't have exact codes to match\n",
    "memory = \"Student prefers online courses\"  # No \"ONLINE-001\" code\n",
    "\n",
    "# vs. Course catalog\n",
    "course = {\n",
    "    \"course_code\": \"CS101\",  # ‚Üê Exact code for keyword search\n",
    "    \"department\": \"Computer Science\",  # ‚Üê Exact category\n",
    "    \"title\": \"Introduction to Programming\"\n",
    "}\n",
    "```\n",
    "\n",
    "**3. Small Dataset Per User**\n",
    "```python\n",
    "# Typical user has <100 memories\n",
    "# Vector search is fast enough\n",
    "# No need for keyword optimization\n",
    "\n",
    "# vs. Course catalog with 1000s of courses\n",
    "# Hybrid search improves performance and precision\n",
    "```\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "**Memory Search (Semantic):**\n",
    "```python\n",
    "# Query: \"What are the student's interests?\"\n",
    "results = await memory_client.search_long_term_memory(\n",
    "    text=\"What are the student's interests?\",\n",
    "    user_id=UserId(eq=student_id)\n",
    ")\n",
    "# Finds: \"Interested in machine learning and AI\"\n",
    "#        \"Enjoys data science projects\"\n",
    "# ‚úÖ Semantic understanding matches conceptually\n",
    "```\n",
    "\n",
    "**Course Search (Hybrid):**\n",
    "```python\n",
    "# Query: \"beginner CS programming courses\"\n",
    "# Semantic: Finds conceptually similar courses\n",
    "# Keyword: Filters by department=\"Computer Science\", difficulty=\"Beginner\"\n",
    "# Hybrid: Best of both worlds!\n",
    "```\n",
    "\n",
    "### When Would Memory Need Hybrid Search?\n",
    "\n",
    "You'd add keyword/hybrid search to memories if:\n",
    "- ‚ùå Memories contained exact codes/IDs to match\n",
    "- ‚ùå Users searched for specific technical terms\n",
    "- ‚ùå Dataset was huge (millions of memories per user)\n",
    "\n",
    "**But:** None of these apply to conversational memory!\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**Different data types need different search strategies:**\n",
    "\n",
    "```\n",
    "Conversational Data (Memories)\n",
    "    ‚Üì\n",
    "Natural language content\n",
    "    ‚Üì\n",
    "Semantic search only ‚úÖ\n",
    "\n",
    "Structured Catalog (Courses)\n",
    "    ‚Üì\n",
    "Codes + descriptions + metadata\n",
    "    ‚Üì\n",
    "Hybrid search (semantic + keyword) ‚úÖ\n",
    "\n",
    "Reference Data (Course Details)\n",
    "    ‚Üì\n",
    "Fetched by ID only\n",
    "    ‚Üì\n",
    "No search needed (plain keys) ‚úÖ\n",
    "```\n",
    "\n",
    "This is why Module 2 teaches all search types, but Module 4 uses semantic-only for memories!\n",
    "\n",
    "---\n",
    "\n",
    "## üè∑Ô∏è Advanced: Topics and Filtering\n",
    "\n",
    "Topics help organize and filter memories. Let's explore how to use them effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de1264c1d1bfea80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:18.957517Z",
     "iopub.status.busy": "2025-12-09T19:56:18.957348Z",
     "iopub.status.idle": "2025-12-09T19:56:18.973233Z",
     "shell.execute_reply": "2025-12-09T19:56:18.972567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üè∑Ô∏è  TOPICS AND FILTERING DEMO\n",
      "================================================================================\n",
      "\n",
      "üìç Storing Memories with Topics\n",
      "--------------------------------------------------------------------------------\n",
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers online courses\n",
      "      Topics: preferences, course_format\n",
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student's major is Computer Science\n",
      "      Topics: academic_info, major\n",
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student wants to graduate in Spring 2026\n",
      "      Topics: goals, graduation\n",
      "14:56:18 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Student prefers morning classes\n",
      "      Topics: preferences, schedule\n"
     ]
    }
   ],
   "source": [
    "topics_student_id = \"sarah_chen\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üè∑Ô∏è  TOPICS AND FILTERING DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìç Storing Memories with Topics\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Define memories with their topics\n",
    "memories_with_topics = [\n",
    "    (\"Student prefers online courses\", [\"preferences\", \"course_format\"]),\n",
    "    (\"Student's major is Computer Science\", [\"academic_info\", \"major\"]),\n",
    "    (\"Student wants to graduate in Spring 2026\", [\"goals\", \"graduation\"]),\n",
    "    (\"Student prefers morning classes\", [\"preferences\", \"schedule\"]),\n",
    "]\n",
    "\n",
    "# Store each memory\n",
    "for memory_text, topics in memories_with_topics:\n",
    "    memory_record = ClientMemoryRecord(\n",
    "        text=memory_text,\n",
    "        user_id=topics_student_id,\n",
    "        memory_type=\"semantic\",\n",
    "        topics=topics,\n",
    "    )\n",
    "    await memory_client.create_long_term_memory([memory_record])\n",
    "    print(f\"   ‚úÖ {memory_text}\")\n",
    "    print(f\"      Topics: {', '.join(topics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726735269a05be58",
   "metadata": {},
   "source": [
    "### Filter memories by type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a752aee40f46df64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:18.975303Z",
     "iopub.status.busy": "2025-12-09T19:56:18.975141Z",
     "iopub.status.idle": "2025-12-09T19:56:19.230484Z",
     "shell.execute_reply": "2025-12-09T19:56:19.229832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Filtering by Memory Type: Semantic\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:19 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 10 semantic memories:\n",
      "   1. Student is preparing for a career in AI research\n",
      "      Topics: career, goals\n",
      "   2. The student prefers online courses over in-person classes.\n",
      "      Topics: preferences, course_format, academic_info\n",
      "   3. Student is currently taking Linear Algebra\n",
      "      Topics: preferences, academic_info\n",
      "   4. Student's major is Computer Science\n",
      "      Topics: academic_info, major\n",
      "   5. Student prefers morning classes\n",
      "      Topics: preferences, schedule\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Topics enable organized, filterable memory management!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìç Filtering by Memory Type: Semantic\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "from agent_memory_client.filters import MemoryType, UserId\n",
    "\n",
    "# Search for all semantic memories\n",
    "results = await memory_client.search_long_term_memory(\n",
    "    text=\"\",  # Empty query returns all\n",
    "    user_id=UserId(eq=topics_student_id),\n",
    "    memory_type=MemoryType(eq=\"semantic\"),\n",
    "    limit=10,\n",
    ")\n",
    "\n",
    "print(f\"   Found {len(results.memories)} semantic memories:\")\n",
    "for i, memory in enumerate(results.memories[:5], 1):\n",
    "    topics_str = \", \".join(memory.topics) if memory.topics else \"none\"\n",
    "    print(f\"   {i}. {memory.text}\")\n",
    "    print(f\"      Topics: {topics_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Topics enable organized, filterable memory management!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46723832e273fad5",
   "metadata": {},
   "source": [
    "### üéØ Why Topics Matter\n",
    "\n",
    "**Organization:**\n",
    "- Group related memories together\n",
    "- Easy to find memories by category\n",
    "\n",
    "**Filtering:**\n",
    "- Search within specific topics\n",
    "- Filter by memory type (semantic, episodic, message)\n",
    "\n",
    "**Best Practices:**\n",
    "- Use consistent topic names\n",
    "- Keep topics broad enough to be useful\n",
    "- Common topics: `preferences`, `academic_info`, `goals`, `schedule`, `courses`\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Cross-Session Memory Persistence\n",
    "\n",
    "Let's verify that memories persist across sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3a39f026cd2f545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:19.232302Z",
     "iopub.status.busy": "2025-12-09T19:56:19.232141Z",
     "iopub.status.idle": "2025-12-09T19:56:19.459812Z",
     "shell.execute_reply": "2025-12-09T19:56:19.458789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîÑ CROSS-SESSION MEMORY PERSISTENCE DEMO\n",
      "================================================================================\n",
      "\n",
      "üìç SESSION 1: Storing Memories\n",
      "--------------------------------------------------------------------------------\n",
      "14:56:19 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Stored: Student is interested in machine learning and AI\n",
      "\n",
      "üìç SESSION 2: New Session, Same Student\n",
      "--------------------------------------------------------------------------------\n",
      "   üîÑ New session started for the same student\n",
      "\n",
      "   üîç Searching: 'What are the student's interests?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:19 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ‚úÖ Memories accessible from new session:\n",
      "      1. Student is interested in machine learning and AI\n",
      "      2. Student's major is Computer Science\n",
      "      3. Student's major is Computer Science with focus on AI/ML\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Long-term memories persist across sessions!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cross_session_student_id = \"sarah_chen\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîÑ CROSS-SESSION MEMORY PERSISTENCE DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìç SESSION 1: Storing Memories\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "memory_record = ClientMemoryRecord(\n",
    "    text=\"Student is interested in machine learning and AI\",\n",
    "    user_id=cross_session_student_id,\n",
    "    memory_type=\"semantic\",\n",
    "    topics=[\"interests\", \"AI\"],\n",
    ")\n",
    "await memory_client.create_long_term_memory([memory_record])\n",
    "print(\"   ‚úÖ Stored: Student is interested in machine learning and AI\")\n",
    "\n",
    "print(\"\\nüìç SESSION 2: New Session, Same Student\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create a new memory client (simulating a new session)\n",
    "new_session_config = MemoryClientConfig(\n",
    "    base_url=AGENT_MEMORY_URL,\n",
    "    default_namespace=\"redis_university\",\n",
    ")\n",
    "new_session_client = MemoryAPIClient(config=new_session_config)\n",
    "\n",
    "print(\"   üîÑ New session started for the same student\")\n",
    "\n",
    "print(\"\\n   üîç Searching: 'What are the student's interests?'\")\n",
    "cross_session_results = await new_session_client.search_long_term_memory(\n",
    "    text=\"What are the student's interests?\",\n",
    "    user_id=UserId(eq=cross_session_student_id),\n",
    "    limit=3,\n",
    ")\n",
    "\n",
    "if cross_session_results.memories:\n",
    "    print(f\"\\n   ‚úÖ Memories accessible from new session:\")\n",
    "    for i, memory in enumerate(cross_session_results.memories[:3], 1):\n",
    "        print(f\"      {i}. {memory.text}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No memories found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Long-term memories persist across sessions!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1a0b5c218d7d2",
   "metadata": {},
   "source": [
    "### üéØ Cross-Session Persistence\n",
    "\n",
    "**What We Demonstrated:**\n",
    "- **Session 1:** Stored memories about student interests\n",
    "- **Session 2:** Created new client (simulating new session)\n",
    "- **Result:** Memories from Session 1 are accessible in Session 2\n",
    "\n",
    "**Why This Matters:**\n",
    "- Users don't have to repeat themselves\n",
    "- Personalization works across days, weeks, months\n",
    "- Knowledge accumulates over time\n",
    "\n",
    "**Contrast with Working Memory:**\n",
    "- Working memory: Session-scoped (persists within the session, like ChatGPT conversations)\n",
    "- Long-term memory: User-scoped (persists across all sessions indefinitely)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4dabaef45a0acf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 3: Memory-Enhanced RAG\n",
    "\n",
    "Now let's combine everything we've learned: working memory, long-term memory, and the hierarchical RAG system from Module 2.\n",
    "\n",
    "### The Complete Pattern\n",
    "\n",
    "```\n",
    "1. Load working memory (conversation history)\n",
    "2. Search long-term memory (user facts)\n",
    "3. Hierarchical RAG search (summaries ‚Üí details)\n",
    "4. Assemble all four context types\n",
    "5. Generate response\n",
    "6. Save working memory (updated conversation)\n",
    "```\n",
    "\n",
    "This gives us **stateful, personalized, context-aware conversations**.\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ Before: Stateless RAG (Module 2 Approach)\n",
    "\n",
    "Let's first recall how Module 2's stateless RAG worked, and see its limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5576bc298f2db2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:19.462134Z",
     "iopub.status.busy": "2025-12-09T19:56:19.461950Z",
     "iopub.status.idle": "2025-12-09T19:56:24.043386Z",
     "shell.execute_reply": "2025-12-09T19:56:24.042433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üö´ STATELESS RAG DEMO\n",
      "================================================================================\n",
      "\n",
      "üë§ User: I'm interested in machine learning courses\n",
      "\n",
      "14:56:19 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I'm interested in machine learning courses' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:19 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:19 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: I'm interested in machine learning courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:19 redis_context_course.hierarchical_manager INFO   Fetched 2 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:19 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 3 summaries, 2 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent: Based on your interest in machine learning, data science, and algorithms, here are the recommended courses from the available list:\n",
      "\n",
      "1. **CS012: Machine Learning Fundamentals**\n",
      "   - This course provides a comprehensive introduction to machine learning algorithms and applications. It covers both supervised and unsupervised learning, neural networks, and deep learning. It's a great fit for your interests in machine learning and data science. However, please note that it requires prerequisites CS002 and MATH020, which you may need to complete first.\n",
      "\n",
      "2. **CS013: Deep Learning and Neural Networks**\n",
      "   - This course focuses on advanced neural network architectures and deep learning techniques, which aligns well with your interest in machine learning. It covers topics like convolutional neural networks, recurrent networks, and transformers. The prerequisite for this course is CS010.\n",
      "\n",
      "3. **MATH022: Linear Algebra for Machine Learning**\n",
      "   - While not a machine learning course per se, this course covers essential mathematical concepts like vectors, matrices, and linear transformations that are foundational for understanding machine learning algorithms. It could be a valuable addition to your studies in machine learning.\n",
      "\n",
      "These courses should align well with your academic background and interests. Make sure to check the prerequisites and plan accordingly.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üö´ STATELESS RAG DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stateless_query_1 = \"I'm interested in machine learning courses\"\n",
    "print(f\"\\nüë§ User: {stateless_query_1}\\n\")\n",
    "\n",
    "# Search courses using hierarchical retrieval\n",
    "stateless_summaries, stateless_details = await hierarchical_manager.hierarchical_search(\n",
    "    query=stateless_query_1,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "# Assemble context (System + User + Retrieved only - NO conversation history)\n",
    "stateless_system_prompt = \"\"\"You are a Redis University course advisor.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- ONLY discuss and recommend courses from the \"Relevant Courses\" list provided below\n",
    "- Do NOT mention, suggest, or make up any courses that are not in the provided list\n",
    "- If the available courses don't perfectly match the request, recommend the best options from what IS available\"\"\"\n",
    "\n",
    "stateless_user_context = f\"\"\"Student: {sarah.name}\n",
    "Major: {sarah.major}\n",
    "Interests: {', '.join(sarah.interests)}\n",
    "Completed: {', '.join(sarah.completed_courses)}\n",
    "\"\"\"\n",
    "\n",
    "# Use context assembler for progressive disclosure\n",
    "stateless_retrieved_context = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=stateless_summaries,\n",
    "    details=stateless_details,\n",
    "    query=stateless_query_1\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "stateless_messages_1 = [\n",
    "    SystemMessage(content=stateless_system_prompt),\n",
    "    HumanMessage(\n",
    "        content=f\"{stateless_user_context}\\n\\n{stateless_retrieved_context}\\n\\nQuery: {stateless_query_1}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "stateless_response_1 = llm.invoke(stateless_messages_1).content\n",
    "print(f\"ü§ñ Agent: {stateless_response_1}\")\n",
    "\n",
    "# ‚ùå No conversation history stored\n",
    "# ‚ùå Next query won't remember this interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6347c1cafa2a2",
   "metadata": {},
   "source": [
    "### Query 2: Follow-up with pronoun reference (fails)\n",
    "\n",
    "Now let's try a follow-up that requires conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4dc4ac3144b6fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:24.045603Z",
     "iopub.status.busy": "2025-12-09T19:56:24.045407Z",
     "iopub.status.idle": "2025-12-09T19:56:25.856741Z",
     "shell.execute_reply": "2025-12-09T19:56:25.855934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë§ User: What are the prerequisites for the first one?\n",
      "   Note: 'the first one' refers to the first course from Query 1\n",
      "\n",
      "14:56:24 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What are the prerequisites for the first one?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:24 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:24 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: What are the prerequisites for the first one?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:24 redis_context_course.hierarchical_manager INFO   Fetched 2 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:24 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 3 summaries, 2 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:25 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: The first course listed is MATH023: Pre-Calculus. There are no specific prerequisites mentioned for this course. It is a beginner-level course designed to build the algebraic and trigonometric foundations needed for success in calculus.\n",
      "\n",
      "‚ùå Agent can't resolve 'the first one' - no conversation history!\n"
     ]
    }
   ],
   "source": [
    "stateless_query_2 = \"What are the prerequisites for the first one?\"\n",
    "print(f\"üë§ User: {stateless_query_2}\")\n",
    "print(\"   Note: 'the first one' refers to the first course from Query 1\\n\")\n",
    "\n",
    "# Search courses (will search for \"prerequisites first one\" - not helpful)\n",
    "stateless_summaries_2, stateless_details_2 = await hierarchical_manager.hierarchical_search(\n",
    "    query=stateless_query_2,\n",
    "    summary_limit=3,\n",
    "    detail_limit=2\n",
    ")\n",
    "\n",
    "# Assemble context (NO conversation history from Query 1)\n",
    "stateless_retrieved_context_2 = context_assembler.assemble_hierarchical_context(\n",
    "    summaries=stateless_summaries_2,\n",
    "    details=stateless_details_2,\n",
    "    query=stateless_query_2\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "stateless_messages_2 = [\n",
    "    SystemMessage(content=stateless_system_prompt),\n",
    "    HumanMessage(\n",
    "        content=f\"{stateless_user_context}\\n\\n{stateless_retrieved_context_2}\\n\\nQuery: {stateless_query_2}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "stateless_response_2 = llm.invoke(stateless_messages_2).content\n",
    "print(f\"\\nü§ñ Agent: {stateless_response_2}\")\n",
    "print(\"\\n‚ùå Agent can't resolve 'the first one' - no conversation history!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b0e0ae54a4b70",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "**Query 1:** \"I'm interested in machine learning courses\"\n",
    "- ‚úÖ Works fine - searches and returns ML courses\n",
    "\n",
    "**Query 2:** \"What are the prerequisites for **the first one**?\"\n",
    "- ‚ùå **Fails** - Agent doesn't know what \"the first one\" refers to\n",
    "- ‚ùå No conversation history stored\n",
    "- ‚ùå Each query is completely independent\n",
    "\n",
    "**The Problem:** Natural conversation requires context from previous turns.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ After: Memory-Enhanced RAG\n",
    "\n",
    "Now let's add memory to enable natural conversations.\n",
    "\n",
    "### Helper Function: Memory-Enhanced RAG with Hierarchical Retrieval\n",
    "\n",
    "This function combines all four context types with hierarchical retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ada2c98ee43badd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:25.858338Z",
     "iopub.status.busy": "2025-12-09T19:56:25.858248Z",
     "iopub.status.idle": "2025-12-09T19:56:25.866427Z",
     "shell.execute_reply": "2025-12-09T19:56:25.866039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory-enhanced RAG function created\n",
      "   Uses: Working memory + Long-term memory + Hierarchical RAG\n"
     ]
    }
   ],
   "source": [
    "async def memory_enhanced_rag_query(\n",
    "    user_query: str,\n",
    "    student_profile: StudentProfile,\n",
    "    session_id: str,\n",
    "    summary_limit: int = 3,\n",
    "    detail_limit: int = 2\n",
    ") -> str:\n",
    "    \"\"\"Generate response using memory-enhanced RAG with hierarchical retrieval\"\"\"\n",
    "\n",
    "    if not MEMORY_SERVER_AVAILABLE:\n",
    "        return \"‚ö†Ô∏è Memory Server not available\"\n",
    "\n",
    "    from agent_memory_client.filters import UserId\n",
    "\n",
    "    student_id = student_profile.email.split(\"@\")[0]\n",
    "\n",
    "    # 1. Load working memory (conversation history)\n",
    "    _, working_memory = await memory_client.get_or_create_working_memory(\n",
    "        session_id=session_id, user_id=student_id, model_name=\"gpt-4o\"\n",
    "    )\n",
    "\n",
    "    # Build conversation messages\n",
    "    conversation_messages = []\n",
    "    for msg in working_memory.messages:\n",
    "        if msg.role == \"user\":\n",
    "            conversation_messages.append(HumanMessage(content=msg.content))\n",
    "        elif msg.role == \"assistant\":\n",
    "            conversation_messages.append(AIMessage(content=msg.content))\n",
    "\n",
    "    # 2. Search long-term memory (user facts)\n",
    "    longterm_results = await memory_client.search_long_term_memory(\n",
    "        text=user_query, user_id=UserId(eq=student_id), limit=5\n",
    "    )\n",
    "    longterm_memories = (\n",
    "        [m.text for m in longterm_results.memories] if longterm_results.memories else []\n",
    "    )\n",
    "\n",
    "    # 3. Hierarchical RAG search (summaries ‚Üí details)\n",
    "    summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "        query=user_query,\n",
    "        summary_limit=summary_limit,\n",
    "        detail_limit=detail_limit\n",
    "    )\n",
    "\n",
    "    # 4. Assemble all four context types\n",
    "    # System Context\n",
    "    system_prompt = \"\"\"You are a Redis University course advisor.\n",
    "\n",
    "Your role:\n",
    "- Help students find and enroll in courses from our catalog\n",
    "- Provide personalized recommendations based on available courses\n",
    "- Answer questions about courses, prerequisites, schedules\n",
    "\n",
    "CRITICAL RULES:\n",
    "- You can ONLY recommend courses that appear in the \"Relevant Courses\" list below\n",
    "- Do NOT suggest courses that are not in the \"Relevant Courses\" list\n",
    "- Use conversation history to resolve references (\"it\", \"that course\", \"the first one\")\n",
    "- Use long-term memories to personalize your recommendations\n",
    "- Be helpful, supportive, and encouraging\"\"\"\n",
    "\n",
    "    # User Context (profile + long-term memories)\n",
    "    user_context = f\"\"\"Student Profile:\n",
    "- Name: {student_profile.name}\n",
    "- Major: {student_profile.major}\n",
    "- Year: {student_profile.year}\n",
    "- Interests: {', '.join(student_profile.interests)}\n",
    "- Completed: {', '.join(student_profile.completed_courses)}\n",
    "- Current: {', '.join(student_profile.current_courses)}\n",
    "- Preferred Format: {student_profile.preferred_format.value}\n",
    "- Preferred Difficulty: {student_profile.preferred_difficulty.value}\"\"\"\n",
    "\n",
    "    if longterm_memories:\n",
    "        user_context += f\"\\n\\nLong-term Memories:\\n\" + \"\\n\".join(\n",
    "            [f\"- {m}\" for m in longterm_memories]\n",
    "        )\n",
    "\n",
    "    # Retrieved Context (hierarchical)\n",
    "    retrieved_context = context_assembler.assemble_hierarchical_context(\n",
    "        summaries=summaries,\n",
    "        details=details,\n",
    "        query=user_query\n",
    "    )\n",
    "\n",
    "    # 5. Build messages and generate response\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.extend(conversation_messages)  # Conversation Context\n",
    "    messages.append(\n",
    "        HumanMessage(\n",
    "            content=f\"{user_context}\\n\\n{retrieved_context}\\n\\nQuery: {user_query}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(messages).content\n",
    "\n",
    "    # 6. Save working memory (updated conversation)\n",
    "    working_memory.messages.extend(\n",
    "        [\n",
    "            MemoryMessage(role=\"user\", content=user_query),\n",
    "            MemoryMessage(role=\"assistant\", content=response),\n",
    "        ]\n",
    "    )\n",
    "    await memory_client.put_working_memory(\n",
    "        session_id=session_id,\n",
    "        memory=working_memory,\n",
    "        user_id=student_id,\n",
    "        model_name=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"‚úÖ Memory-enhanced RAG function created\")\n",
    "print(\"   Uses: Working memory + Long-term memory + Hierarchical RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5c1bc501f416",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Complete Demo: Memory-Enhanced RAG\n",
    "\n",
    "Now let's test the complete system with a multi-turn conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "643a19d378897a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:25.868103Z",
     "iopub.status.busy": "2025-12-09T19:56:25.867996Z",
     "iopub.status.idle": "2025-12-09T19:56:25.870323Z",
     "shell.execute_reply": "2025-12-09T19:56:25.869776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ MEMORY-ENHANCED RAG DEMO\n",
      "================================================================================\n",
      "\n",
      "üë§ Student: Sarah Chen\n",
      "üìß Session: complete_demo_22a1e06e\n"
     ]
    }
   ],
   "source": [
    "# Set up demo session\n",
    "demo_session_id = f\"complete_demo_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ MEMORY-ENHANCED RAG DEMO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüë§ Student: {sarah.name}\")\n",
    "print(f\"üìß Session: {demo_session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833d0da7f03778b",
   "metadata": {},
   "source": [
    "### Turn 1: Initial Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ba1e4fc900497b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:25.871623Z",
     "iopub.status.busy": "2025-12-09T19:56:25.871509Z",
     "iopub.status.idle": "2025-12-09T19:56:30.526251Z",
     "shell.execute_reply": "2025-12-09T19:56:30.525264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 1: Initial Query\n",
      "================================================================================\n",
      "\n",
      "üë§ User: I'm interested in machine learning courses\n",
      "14:56:25 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:25 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:26 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:26 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'I'm interested in machine learning courses' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:26 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:26 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: I'm interested in machine learning courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:26 redis_context_course.hierarchical_manager INFO   Fetched 2 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:26 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 3 summaries, 2 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:30 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:30 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Hi Sarah! It's great to see your continued interest in machine learning. Based on your profile and preferences, I recommend considering the following course:\n",
      "\n",
      "### CS013: Deep Learning and Neural Networks\n",
      "- **Format**: Online\n",
      "- **Level**: Advanced\n",
      "- **Instructor**: Lisa Hensley\n",
      "- **Prerequisites**: CS010\n",
      "\n",
      "This course dives into advanced neural network architectures and deep learning techniques, which aligns well with your interest in machine learning and data science. Since you prefer online courses, this format will suit your needs perfectly. Additionally, the course covers a range of topics from convolutional neural networks to generative models, providing a comprehensive understanding of deep learning.\n",
      "\n",
      "If you have any questions about the course or need further assistance, feel free to ask!\n",
      "\n",
      "‚úÖ Conversation saved to working memory\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 1: Initial Query\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "demo_query_1 = \"I'm interested in machine learning courses\"\n",
    "print(f\"\\nüë§ User: {demo_query_1}\")\n",
    "\n",
    "demo_response_1 = await memory_enhanced_rag_query(demo_query_1, sarah, demo_session_id)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {demo_response_1}\")\n",
    "print(\"\\n‚úÖ Conversation saved to working memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373996c88eb205b9",
   "metadata": {},
   "source": [
    "### Turn 2: Follow-up with Pronoun Reference\n",
    "\n",
    "Now let's ask about \"the first one\" - a reference that requires conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9be49bea743df8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:30.528764Z",
     "iopub.status.busy": "2025-12-09T19:56:30.528644Z",
     "iopub.status.idle": "2025-12-09T19:56:36.705311Z",
     "shell.execute_reply": "2025-12-09T19:56:36.704544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 2: Follow-up with Pronoun Reference\n",
      "================================================================================\n",
      "\n",
      "üë§ User: What are the prerequisites for the first one?\n",
      "   Note: 'the first one' refers to the first course mentioned in Turn 1\n",
      "14:56:30 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:30 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:30 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'What are the prerequisites for the first one?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:31 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:31 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: What are the prerequisites for the first one?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:31 redis_context_course.hierarchical_manager INFO   Fetched 2 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:31 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 3 summaries, 2 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:36 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:36 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: The first course listed in your search results is **MATH023: Pre-Calculus**. This course is designed to build the algebraic and trigonometric foundations needed for success in calculus. It doesn't have any specific prerequisites, making it accessible if you're looking to strengthen your mathematical background, which can be beneficial for machine learning courses.\n",
      "\n",
      "Since you're interested in machine learning and have completed CS101 and CS201, you might want to consider this course if you feel the need to reinforce your math skills, especially since you're currently taking MATH301. This could be a good step towards meeting the prerequisites for more advanced machine learning courses in the future.\n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask!\n",
      "\n",
      "‚úÖ Agent resolved 'the first one' using conversation history!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 2: Follow-up with Pronoun Reference\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "demo_query_2 = \"What are the prerequisites for the first one?\"\n",
    "print(f\"\\nüë§ User: {demo_query_2}\")\n",
    "print(\"   Note: 'the first one' refers to the first course mentioned in Turn 1\")\n",
    "\n",
    "demo_response_2 = await memory_enhanced_rag_query(demo_query_2, sarah, demo_session_id)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {demo_response_2}\")\n",
    "print(\"\\n‚úÖ Agent resolved 'the first one' using conversation history!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa82d08b39bb6c",
   "metadata": {},
   "source": [
    "### Turn 3: Another Follow-up\n",
    "\n",
    "Let's ask if the student meets the prerequisites mentioned in Turn 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "199a67faea2fcc7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T19:56:36.707401Z",
     "iopub.status.busy": "2025-12-09T19:56:36.707260Z",
     "iopub.status.idle": "2025-12-09T19:56:43.467206Z",
     "shell.execute_reply": "2025-12-09T19:56:43.465722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìç TURN 3: Another Follow-up\n",
      "================================================================================\n",
      "\n",
      "üë§ User: Do I meet those prerequisites?\n",
      "   Note: 'those prerequisites' refers to prerequisites from Turn 2\n",
      "14:56:36 httpx INFO   HTTP Request: GET http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&namespace=redis_university&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:37 httpx INFO   HTTP Request: POST http://localhost:8088/v1/long-term-memory/search?optimize_query=false \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:37 redis_context_course.hierarchical_manager INFO   Hierarchical search: 'Do I meet those prerequisites?' (summaries=3, details=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:37 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:37 redis_context_course.hierarchical_manager INFO   Found 3 course summaries for query: Do I meet those prerequisites?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:37 redis_context_course.hierarchical_manager INFO   Fetched 2 course details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:37 redis_context_course.hierarchical_manager INFO   Hierarchical search complete: 3 summaries, 2 details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:43 httpx INFO   HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:56:43 httpx INFO   HTTP Request: PUT http://localhost:8088/v1/working-memory/complete_demo_22a1e06e?user_id=sarah.chen&model_name=gpt-4o \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agent: Based on your completed courses and current enrollment, let's review the prerequisites for the courses you're interested in:\n",
      "\n",
      "1. **CS012: Machine Learning Fundamentals** - Prerequisites: CS002\n",
      "   - You have not completed CS002, so you would need to take that course first.\n",
      "\n",
      "2. **MATH022: Linear Algebra for Machine Learning** - Prerequisites: MATH020\n",
      "   - You have not completed MATH020, so you would need to take that course first.\n",
      "\n",
      "3. **CS013: Deep Learning and Neural Networks** - Prerequisites: CS010\n",
      "   - You have not completed CS010, so you would need to take that course first.\n",
      "\n",
      "For the courses listed in your search results:\n",
      "\n",
      "- **BIO037: Ecology and Environmental Science** requires BIO032, which is not related to your current focus on machine learning.\n",
      "- **MATH024: Calculus I** requires MATH021, which you haven't completed, but it could be a good step if you're looking to strengthen your math skills for future machine learning courses.\n",
      "- **MATH023: Pre-Calculus** has no prerequisites and could be a good foundational course if you want to build up to more advanced math courses.\n",
      "\n",
      "If you're looking to meet the prerequisites for machine learning courses, focusing on completing CS002, MATH020, and CS010 would be beneficial. Let me know if you need more guidance or have any other questions!\n",
      "\n",
      "‚úÖ Agent resolved 'those prerequisites' and checked student's transcript!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ DEMO COMPLETE: Memory-enhanced RAG enables natural conversations!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìç TURN 3: Another Follow-up\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "demo_query_3 = \"Do I meet those prerequisites?\"\n",
    "print(f\"\\nüë§ User: {demo_query_3}\")\n",
    "print(\"   Note: 'those prerequisites' refers to prerequisites from Turn 2\")\n",
    "\n",
    "demo_response_3 = await memory_enhanced_rag_query(demo_query_3, sarah, demo_session_id)\n",
    "\n",
    "print(f\"\\nü§ñ Agent: {demo_response_3}\")\n",
    "print(\"\\n‚úÖ Agent resolved 'those prerequisites' and checked student's transcript!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ DEMO COMPLETE: Memory-enhanced RAG enables natural conversations!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0bda325b89750",
   "metadata": {},
   "source": [
    "### üéØ What Just Happened?\n",
    "\n",
    "**Turn 1:** \"I'm interested in machine learning courses\"\n",
    "- System uses hierarchical search (summaries ‚Üí details)\n",
    "- Finds ML-related courses\n",
    "- Responds with recommendations\n",
    "- **Saves conversation to working memory**\n",
    "\n",
    "**Turn 2:** \"What are the prerequisites for **the first one**?\"\n",
    "- System loads working memory (Turn 1)\n",
    "- Resolves \"the first one\" ‚Üí first course mentioned in Turn 1\n",
    "- Responds with prerequisites\n",
    "- **Saves updated conversation**\n",
    "\n",
    "**Turn 3:** \"Do I meet **those prerequisites**?\"\n",
    "- System loads working memory (Turns 1-2)\n",
    "- Resolves \"those prerequisites\" ‚Üí prerequisites from Turn 2\n",
    "- Checks student's completed courses from profile\n",
    "- Responds with personalized assessment\n",
    "\n",
    "**Key Insight:** Memory transforms stateless RAG into stateful, personalized, context-aware conversations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d53baf5137ef3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Before vs. After Comparison\n",
    "\n",
    "Let's visualize the difference between stateless and memory-enhanced RAG.\n",
    "\n",
    "### **Stateless RAG (Module 2):**\n",
    "\n",
    "```\n",
    "Query 1: \"I'm interested in ML courses\"\n",
    "  ‚Üí ‚úÖ Works (searches and returns courses)\n",
    "\n",
    "Query 2: \"What are the prerequisites for the first one?\"\n",
    "  ‚Üí ‚ùå Fails (no conversation history)\n",
    "  ‚Üí Agent: \"Which course are you referring to?\"\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ‚ùå No conversation continuity\n",
    "- ‚ùå Can't resolve references\n",
    "- ‚ùå Each query is independent\n",
    "- ‚ùå Poor user experience\n",
    "\n",
    "### **Memory-Enhanced RAG (This Module):**\n",
    "\n",
    "```\n",
    "Query 1: \"I'm interested in ML courses\"\n",
    "  ‚Üí ‚úÖ Works (searches and returns courses)\n",
    "  ‚Üí Saves to working memory\n",
    "\n",
    "Query 2: \"What are the prerequisites for the first one?\"\n",
    "  ‚Üí ‚úÖ Works (loads conversation history)\n",
    "  ‚Üí Resolves \"the first one\" ‚Üí first course from Query 1\n",
    "  ‚Üí Responds with prerequisites\n",
    "  ‚Üí Saves updated conversation\n",
    "\n",
    "Query 3: \"Do I meet those prerequisites?\"\n",
    "  ‚Üí ‚úÖ Works (loads conversation history)\n",
    "  ‚Üí Resolves \"those prerequisites\" ‚Üí prerequisites from Query 2\n",
    "  ‚Üí Checks student transcript\n",
    "  ‚Üí Responds with personalized answer\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Conversation continuity\n",
    "- ‚úÖ Reference resolution\n",
    "- ‚úÖ Personalization\n",
    "- ‚úÖ Natural user experience\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### **1. Memory Transforms RAG**\n",
    "\n",
    "**Without Memory (Module 2):**\n",
    "- Stateless queries\n",
    "- No conversation continuity\n",
    "- Limited to 3 context types (System, User, Retrieved)\n",
    "\n",
    "**With Memory (This Module):**\n",
    "- Stateful conversations\n",
    "- Reference resolution\n",
    "- All 4 context types (System, User, Conversation, Retrieved)\n",
    "\n",
    "### **2. Two Types of Memory Work Together**\n",
    "\n",
    "**Working Memory:**\n",
    "- Session-scoped conversation history\n",
    "- Enables reference resolution\n",
    "- Persists within the session (like ChatGPT conversations)\n",
    "\n",
    "**Long-term Memory:**\n",
    "- User-scoped persistent facts\n",
    "- Enables personalization\n",
    "- Persists indefinitely\n",
    "\n",
    "### **3. Hierarchical Retrieval + Memory**\n",
    "\n",
    "**What We Built:**\n",
    "- Combined hierarchical RAG (summaries ‚Üí details) with memory\n",
    "- Progressive disclosure pattern from Module 2\n",
    "- Memory-enhanced context assembly\n",
    "- All four context types working together\n",
    "\n",
    "**Why This Matters:**\n",
    "- Efficient token usage (progressive disclosure)\n",
    "- Natural conversations (memory)\n",
    "- Personalization (long-term memory)\n",
    "- Foundation for agentic workflows (Module 5)\n",
    "\n",
    "### **4. All Four Context Types**\n",
    "\n",
    "| Context Type | Source | Purpose |\n",
    "|--------------|--------|---------|\n",
    "| **System Context** | Static prompt | Role, instructions, guidelines |\n",
    "| **User Context** | Profile + long-term memories | Personalization |\n",
    "| **Conversation Context** | Working memory | Reference resolution |\n",
    "| **Retrieved Context** | Hierarchical RAG | Relevant information |\n",
    "\n",
    "**Together:** Natural, stateful, personalized conversations\n",
    "\n",
    "**üí° Research Insight (From Module 1):** Context Rot research demonstrates that context structure and organization affect LLM attention. Memory systems that selectively retrieve and organize context outperform systems that dump all available information. This validates our approach: quality over quantity, semantic similarity, and selective retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è Practice Exercises\n",
    "\n",
    "### **Exercise 1: Cross-Session Personalization**\n",
    "\n",
    "Modify the `memory_enhanced_rag_query` function to:\n",
    "1. Store user preferences in long-term memory when mentioned\n",
    "2. Use those preferences in future sessions\n",
    "3. Test with two different sessions for the same student\n",
    "\n",
    "**Hint:** Look for phrases like \"I prefer...\", \"I like...\", \"I want...\" and store them as semantic memories.\n",
    "\n",
    "### **Exercise 2: Memory-Aware Filtering**\n",
    "\n",
    "Enhance the hierarchical search to use long-term memories as filters:\n",
    "1. Search long-term memory for preferences (format, difficulty, schedule)\n",
    "2. Apply those preferences as filters to `hierarchical_manager.hierarchical_search()`\n",
    "3. Compare results with and without memory-aware filtering\n",
    "\n",
    "**Hint:** Use the `filters` parameter in the search methods.\n",
    "\n",
    "### **Exercise 3: Conversation Summarization**\n",
    "\n",
    "Implement a function that summarizes long conversations:\n",
    "1. When working memory exceeds 10 messages, summarize the conversation\n",
    "2. Store the summary in long-term memory\n",
    "3. Clear old messages from working memory (keep only recent 4)\n",
    "4. Test that reference resolution still works with summarized history\n",
    "\n",
    "**Hint:** Use the LLM to generate summaries, then store as semantic memories.\n",
    "\n",
    "### **Exercise 4: Multi-User Memory Management**\n",
    "\n",
    "Create a simple CLI that:\n",
    "1. Supports multiple students (different user IDs)\n",
    "2. Maintains separate working memory per session\n",
    "3. Maintains separate long-term memory per user\n",
    "4. Demonstrates cross-session continuity for each user\n",
    "\n",
    "**Hint:** Use different `session_id` and `user_id` for each student.\n",
    "\n",
    "### **Exercise 5: Memory Search Quality**\n",
    "\n",
    "Experiment with long-term memory search:\n",
    "1. Store 20+ diverse memories for a student\n",
    "2. Try different search queries\n",
    "3. Analyze which memories are retrieved\n",
    "4. Adjust memory text to improve search relevance\n",
    "\n",
    "**Hint:** More specific memory text leads to better semantic search results.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### **What You Learned:**\n",
    "\n",
    "1. **The Grounding Problem** - Why agents need memory to resolve references\n",
    "2. **Working Memory** - Session-scoped conversation history for continuity\n",
    "3. **Long-term Memory** - Cross-session persistent knowledge for personalization\n",
    "4. **Memory Integration** - Combining memory with Module 2's hierarchical RAG system\n",
    "5. **Complete Context Engineering** - All four context types working together\n",
    "6. **Production Architecture** - Using Agent Memory Server for scalable memory\n",
    "\n",
    "### **What You Built:**\n",
    "\n",
    "- ‚úÖ Working memory demo (multi-turn conversations)\n",
    "- ‚úÖ Long-term memory demo (persistent knowledge)\n",
    "- ‚úÖ Complete memory-enhanced RAG system with hierarchical retrieval\n",
    "- ‚úÖ Integration of all four context types\n",
    "\n",
    "### **Key Functions:**\n",
    "\n",
    "- `memory_enhanced_rag_query()` - Complete memory + hierarchical RAG pipeline\n",
    "- Working memory operations - Load, save, update conversation history\n",
    "- Long-term memory operations - Store, search, filter persistent facts\n",
    "\n",
    "### **Architecture Pattern:**\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Load Working Memory (conversation history)\n",
    "    ‚Üì\n",
    "Search Long-term Memory (user facts)\n",
    "    ‚Üì\n",
    "Hierarchical RAG Search (summaries ‚Üí details)\n",
    "    ‚Üì\n",
    "Assemble Context (System + User + Conversation + Retrieved)\n",
    "    ‚Üì\n",
    "Generate Response\n",
    "    ‚Üì\n",
    "Save Working Memory (updated conversation)\n",
    "```\n",
    "\n",
    "### **From Module 2 to Module 4:**\n",
    "\n",
    "**Module 2 (Stateless RAG):**\n",
    "- ‚ùå No conversation history\n",
    "- ‚ùå Each query independent\n",
    "- ‚ùå Can't resolve references\n",
    "- ‚úÖ Retrieves relevant documents\n",
    "- ‚úÖ Progressive disclosure (hierarchical)\n",
    "\n",
    "**Module 4 (Memory-Enhanced RAG):**\n",
    "- ‚úÖ Conversation history (working memory)\n",
    "- ‚úÖ Multi-turn conversations\n",
    "- ‚úÖ Reference resolution\n",
    "- ‚úÖ Persistent user knowledge (long-term memory)\n",
    "- ‚úÖ Personalization across sessions\n",
    "- ‚úÖ Progressive disclosure (hierarchical)\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "**Module 5** will add **tools** and **agentic workflows** using **LangGraph**, completing your journey from context engineering fundamentals to production-ready AI agents.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a **memory-enhanced RAG system** that:\n",
    "- Remembers conversations (working memory)\n",
    "- Accumulates knowledge (long-term memory)\n",
    "- Resolves references naturally\n",
    "- Personalizes responses\n",
    "- Uses progressive disclosure (hierarchical retrieval)\n",
    "- Integrates all four context types\n",
    "\n",
    "**You're now ready for Module 5: Building Agents!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [Agent Memory Server Documentation](https://github.com/redis/agent-memory-server) - Production-ready memory management\n",
    "- [Agent Memory Client](https://pypi.org/project/agent-memory-client/) - Python client for Agent Memory Server\n",
    "- [RedisVL Documentation](https://redisvl.com/) - Redis Vector Library\n",
    "- [LangChain Guide](https://python.langchain.com/docs/modules/memory/) - LangChain memory patterns\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/) - Building agents with LangGraph\n",
    "\n",
    "---\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "**Redis University - Context Engineering Workshop**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef5b878cca12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
