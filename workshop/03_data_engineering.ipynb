{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fab7a26bf5c3a5",
   "metadata": {},
   "source": [
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "# Module 3: Chunking and Data Modeling for RAG\n",
    "\n",
    "## From Basic RAG to Production-Ready Knowledge Bases\n",
    "\n",
    "In Module 2, you built a working RAG system with hierarchical search. Now you'll learn the critical engineering decisions that separate toy demos from production systems: **when and how to chunk your data**.\n",
    "\n",
    "**The Critical Question:** Does my data need chunking?\n",
    "\n",
    "This module teaches you that **chunking is a design choice, not a default step**. Just like database schema design, how you structure your knowledge base dramatically affects retrieval quality, token efficiency, and system performance.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "**1. The \"Don't Chunk\" Strategy:**\n",
    "- When whole-document embedding is the right choice\n",
    "- Why structured records (courses, products, FAQs) often don't need chunking\n",
    "- How to recognize natural retrieval boundaries in your data\n",
    "\n",
    "**2. When Chunking Helps:**\n",
    "- Document types that benefit from chunking (research papers, long-form content)\n",
    "- Research-backed insights: \"Lost in the Middle\", \"Context Rot\"\n",
    "- How chunking improves retrieval precision\n",
    "\n",
    "**3. Chunking Strategies:**\n",
    "- Document-based (structure-aware): Split by sections/headers\n",
    "- Fixed-size (token-based): Using LangChain's RecursiveCharacterTextSplitter\n",
    "- Semantic (meaning-based): Using embeddings to detect topic shifts\n",
    "- Trade-offs and decision framework\n",
    "\n",
    "**4. Data Modeling for RAG:**\n",
    "- The hierarchical pattern: summaries + details\n",
    "- Engineering workflow: Extract ‚Üí Clean ‚Üí Transform ‚Üí Optimize ‚Üí Store\n",
    "- Real-world examples with Redis University course catalog\n",
    "\n",
    "**‚è±Ô∏è Estimated Time:** 60-75 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Module 2: RAG Fundamentals and Implementation\n",
    "- Redis 8 running locally with course data loaded\n",
    "- OpenAI API key set\n",
    "- Understanding of vector embeddings and semantic search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c04ea493c9b4a",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "6a9f0372e941e8e",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Handle both running from workshop/ directory and from project root\n",
    "if Path.cwd().name == \"workshop\":\n",
    "    project_root = Path.cwd().parent\n",
    "else:\n",
    "    project_root = Path.cwd()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables from project root\n",
    "env_path = project_root / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\"OPENAI_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\"\"‚ö†Ô∏è  Missing required environment variables: {', '.join(missing_vars)}\n",
    "\n",
    "Please create a .env file with:\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "REDIS_URL=redis://localhost:6379\n",
    "\"\"\")\n",
    "    sys.exit(1)\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\n",
    "print(\"‚úÖ Environment variables loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d320da647edaf123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import redis\n",
    "import tiktoken\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import hierarchical components (from Module 2)\n",
    "from redis_context_course.hierarchical_manager import HierarchicalCourseManager\n",
    "from redis_context_course.hierarchical_context import HierarchicalContextAssembler\n",
    "\n",
    "# Initialize\n",
    "hierarchical_manager = HierarchicalCourseManager(redis_client=redis.from_url(REDIS_URL, decode_responses=True))\n",
    "context_assembler = HierarchicalContextAssembler()\n",
    "redis_client = redis.from_url(REDIS_URL, decode_responses=True)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Token counter\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efd2fd5842a5e9",
   "metadata": {},
   "source": [
    "## Part 1: Data Modeling - The Foundation of RAG Quality\n",
    "\n",
    "### The Critical First Question: What is My Natural Retrieval Unit?\n",
    "\n",
    "Before thinking about chunking, ask: **\"What is the natural unit of information I want to retrieve?\"**\n",
    "\n",
    "This is similar to database design - you wouldn't store all customer data in one row, and you shouldn't embed all document content in one vector without thinking about retrieval patterns.\n",
    "\n",
    "**Examples of Natural Retrieval Units:**\n",
    "\n",
    "| Domain | Natural Unit | Why |\n",
    "|--------|-------------|-----|\n",
    "| **Course Catalog** | Individual course | Each course is self-contained, complete |\n",
    "| **Product Catalog** | Individual product | All product info should be retrieved together |\n",
    "| **FAQ Database** | Question + Answer pair | Q&A is an atomic unit |\n",
    "| **Research Papers** | Section or paragraph | Different sections answer different queries |\n",
    "| **Legal Contracts** | Clause or section | Need clause-level precision |\n",
    "| **Support Tickets** | Individual ticket | Single issue with context |\n",
    "\n",
    "Let's see this in practice with our course catalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5125abde3b3ad",
   "metadata": {},
   "source": [
    "### Example: Course Catalog - A Natural Retrieval Unit\n",
    "\n",
    "Let's examine a single course to understand why it's already an optimal retrieval unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b763338dc9b9e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:29:08 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "üìä Naive Approach Results:\n",
      "   Courses included: 10\n",
      "   Token count: 1,688\n",
      "   Estimated cost per request: $0.0042\n",
      "\n",
      "   For 100 courses, this would be ~16,880 tokens!\n",
      "\n",
      "\n",
      "üìÑ Sample of raw JSON context:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"course_catalog:01KBQSB5VYQ55YPXGFV2CM0S9S\",\n",
      "    \"course_code\": \"CS004\",\n",
      "    \"title\": \"Database Systems\",\n",
      "    \"description\": \"Design and implementation of database systems. SQL, normalization, transactions, and database administration.\",\n",
      "    \"department\": \"Computer Science\",\n",
      "    \"credits\": 3,\n",
      "    \"difficulty_level\": \"intermediate\",\n",
      "    \"format\": \"online\",\n",
      "    \"instructor\": \"Christopher Adams\",\n",
      "    \"prerequisites\": [],\n",
      "    \"created_at\": \"2025-12-05 17:29:08.824502\",\n",
      "    \"updated_a...\n"
     ]
    }
   ],
   "source": [
    "# Get a sample course to analyze using search\n",
    "sample_courses = await hierarchical_manager.search_summaries(\n",
    "    query=\"programming courses\", limit=3\n",
    ")\n",
    "sample_course = sample_courses[0]  # Get first course\n",
    "\n",
    "# Generate embedding text if not present\n",
    "if not sample_course.embedding_text:\n",
    "    sample_course.generate_embedding_text()\n",
    "\n",
    "# Display the course summary\n",
    "print(f\"\"\"üìö Sample Course: {sample_course.course_code}\n",
    "{'=' * 80}\n",
    "Title: {sample_course.title}\n",
    "Department: {sample_course.department}\n",
    "Level: {sample_course.difficulty_level.value}\n",
    "Credits: {sample_course.credits}\n",
    "Instructor: {sample_course.instructor}\n",
    "\n",
    "Description:\n",
    "{sample_course.short_description}\n",
    "\n",
    "Prerequisites: {', '.join(sample_course.prerequisite_codes) if sample_course.prerequisite_codes else 'None'}\n",
    "Tags: {', '.join(sample_course.tags) if sample_course.tags else 'None'}\n",
    "{'=' * 80}\n",
    "\n",
    "Token count: {count_tokens(sample_course.embedding_text)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0c2bc03a500fb",
   "metadata": {},
   "source": [
    "### Analysis: Why Courses Don't Need Chunking\n",
    "\n",
    "**Semantic Completeness:** ‚úÖ Each course is self-contained\n",
    "- All information about the course is in one record\n",
    "- No cross-references to other sections\n",
    "- Natural boundary exists (one course = one retrieval unit)\n",
    "\n",
    "**Query Patterns:** ‚úÖ Users ask about specific courses or course types\n",
    "- \"What machine learning courses are available?\"\n",
    "- \"Tell me about CS016\"\n",
    "- \"What are the prerequisites for RU102JS?\"\n",
    "\n",
    "**Retrieval Precision:** ‚úÖ Whole-course embedding maximizes relevance\n",
    "- When a user asks about a course, they need ALL the information\n",
    "- Splitting would fragment related information (e.g., separating prerequisites from description)\n",
    "- Each course is already the optimal retrieval unit\n",
    "\n",
    "**Token Efficiency:** ‚úÖ Courses are reasonably sized (~150-200 tokens each)\n",
    "- Not too large (no wasted context)\n",
    "- Not too small (no fragmentation)\n",
    "\n",
    "**Decision:** ‚ùå **Don't chunk course data** - it's already optimally structured!\n",
    "\n",
    "This is the **\"don't chunk\" strategy** - a valid and often optimal choice for structured records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9ef0558e87e03",
   "metadata": {},
   "source": [
    "### The Hierarchical Pattern: A Better Data Model\n",
    "\n",
    "Instead of chunking, we use a **hierarchical pattern** with two tiers:\n",
    "\n",
    "**Tier 1: Summaries (Lightweight)**\n",
    "- Searchable, compact course overviews\n",
    "- Stored in vector index for fast retrieval\n",
    "- ~150-200 tokens each\n",
    "\n",
    "**Tier 2: Details (On-Demand)**\n",
    "- Full course information with all fields\n",
    "- Retrieved only when needed\n",
    "- Stored as plain Redis keys (not in vector index)\n",
    "\n",
    "This is **data modeling**, not chunking - we're structuring data for optimal retrieval patterns.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d10899a005e07b82",
   "metadata": {},
   "source": [
    "# Hierarchical retrieval example\n",
    "query = \"beginner programming courses\"\n",
    "\n",
    "# Tier 1: Search summaries (fast, lightweight)\n",
    "summaries, details = await hierarchical_manager.hierarchical_search(\n",
    "    query=query,\n",
    "    summary_limit=5,  # Get 5 summary matches\n",
    "    detail_limit=3,   # Fetch full details for top 3\n",
    ")\n",
    "\n",
    "print(f\"\"\"üîç Query: \"{query}\"\n",
    "{'=' * 80}\n",
    "\n",
    "üìä Tier 1: Summary Results (5 courses)\n",
    "\"\"\")\n",
    "\n",
    "for i, summary in enumerate(summaries, 1):\n",
    "    print(f\"{i}. {summary.course_code}: {summary.title} ({summary.difficulty_level})\")\n",
    "\n",
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "üìÑ Tier 2: Detailed Information (top 3 courses)\n",
    "\"\"\")\n",
    "\n",
    "for detail in details:\n",
    "    prereq_codes = [p.course_code for p in detail.prerequisites] if detail.prerequisites else []\n",
    "    print(f\"\"\"\n",
    "{detail.course_code}: {detail.title}\n",
    "Department: {detail.department} | Credits: {detail.credits}\n",
    "Prerequisites: {', '.join(prereq_codes) if prereq_codes else 'None'}\n",
    "\n",
    "Description: {detail.full_description[:200]}...\n",
    "\"\"\")\n",
    "\n",
    "# Assemble context\n",
    "context = context_assembler.assemble_hierarchical_context(summaries, details, query)\n",
    "context_tokens = count_tokens(context)\n",
    "\n",
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "üìä Context Statistics:\n",
    "- Summaries: 5 courses\n",
    "- Details: 3 courses\n",
    "- Total tokens: {context_tokens:,}\n",
    "- Retrieval pattern: Hierarchical (summaries + details)\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc0dbcd5795aedec",
   "metadata": {},
   "source": [
    "**Key Takeaway:** For structured records like courses, the hierarchical pattern (summaries + details) is superior to chunking because it respects natural data boundaries and retrieval patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: When Documents DO Need Chunking\n",
    "\n",
    "Now let's look at a completely different type of data: **long-form documents** with multiple distinct topics.\n",
    "\n",
    "### Example: Research Paper\n",
    "\n",
    "Let's create a sample research paper about Redis vector search optimization:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cb0dc692ca494be9",
   "metadata": {},
   "source": [
    "# Create a sample research paper about Redis vector search\n",
    "research_paper = \"\"\"\n",
    "# Optimizing Vector Search Performance in Redis\n",
    "\n",
    "## Abstract\n",
    "This paper presents a comprehensive analysis of vector search optimization techniques in Redis,\n",
    "examining the trade-offs between search quality, latency, and memory usage. We evaluate multiple\n",
    "indexing strategies including HNSW and FLAT indexes across datasets ranging from 10K to 10M vectors.\n",
    "Our results demonstrate that careful index configuration can improve search latency by up to 10x\n",
    "while maintaining 95%+ recall. We also introduce novel compression techniques that reduce memory\n",
    "usage by 75% with minimal impact on search quality.\n",
    "\n",
    "## 1. Introduction\n",
    "Vector databases have become essential infrastructure for modern AI applications, enabling semantic\n",
    "search, recommendation systems, and retrieval-augmented generation (RAG). Redis, traditionally known\n",
    "as an in-memory data structure store, has evolved to support high-performance vector search through\n",
    "the RediSearch module. However, optimizing vector search performance requires understanding complex\n",
    "trade-offs between multiple dimensions: search quality (recall), query latency, memory usage, and\n",
    "index build time.\n",
    "\n",
    "This paper makes three key contributions: (1) A systematic evaluation of HNSW parameter configurations\n",
    "across different dataset sizes and query patterns, (2) Novel compression techniques that reduce memory\n",
    "footprint while preserving search quality, and (3) Practical recommendations for production deployments\n",
    "based on real-world workload analysis.\n",
    "\n",
    "[... continues for several more pages ...]\n",
    "\n",
    "## 2. Background and Related Work\n",
    "Previous work on vector search optimization has focused primarily on algorithmic improvements to\n",
    "approximate nearest neighbor (ANN) search. Malkov and Yashunin (2018) introduced HNSW (Hierarchical\n",
    "Navigable Small World), which has become the de facto standard for high-dimensional vector search.\n",
    "Johnson et al. (2019) developed FAISS, demonstrating that product quantization can significantly\n",
    "reduce memory usage. More recently, Guo et al. (2020) proposed DiskANN for billion-scale search\n",
    "with SSD-based storage.\n",
    "\n",
    "However, these works primarily focus on standalone vector search systems. Our work specifically\n",
    "addresses the unique challenges of integrating vector search into Redis, a multi-model database\n",
    "that must balance vector search performance with other data structure operations.\n",
    "\n",
    "[... continues ...]\n",
    "\n",
    "## 3. Performance Analysis and Results\n",
    "\n",
    "### 3.1 HNSW Configuration Trade-offs\n",
    "\n",
    "Table 1 shows the performance comparison across different HNSW configurations. As M increases from 16 to 64,\n",
    "we observe significant improvements in recall (0.89 to 0.97) but at the cost of increased latency (2.1ms to 8.7ms)\n",
    "and memory usage (1.2GB to 3.8GB). The sweet spot for most real-world workloads is M=32 with ef_construction=200,\n",
    "which achieves 0.94 recall with 4.3ms latency.\n",
    "\n",
    "Table 1: HNSW Performance Comparison\n",
    "| M  | ef_construction | Recall@10 | Latency (ms) | Memory (GB) | Build Time (min) |\n",
    "|----|-----------------|-----------|--------------|-------------|------------------|\n",
    "| 16 | 100            | 0.89      | 2.1          | 1.2         | 8                |\n",
    "| 32 | 200            | 0.94      | 4.3          | 2.1         | 15               |\n",
    "| 64 | 400            | 0.97      | 8.7          | 3.8         | 32               |\n",
    "\n",
    "The data clearly demonstrates the fundamental trade-off between search quality and resource consumption.\n",
    "For applications requiring high recall (>0.95), the increased latency and memory costs are unavoidable.\n",
    "\n",
    "### 3.2 Mathematical Model\n",
    "\n",
    "The recall-latency trade-off can be modeled as a quadratic function of the HNSW parameters:\n",
    "\n",
    "Latency(M, ef) = Œ±¬∑M¬≤ + Œ≤¬∑ef + Œ≥\n",
    "\n",
    "Where:\n",
    "- M = number of connections per layer (controls graph connectivity)\n",
    "- ef = size of dynamic candidate list (controls search breadth)\n",
    "- Œ±, Œ≤, Œ≥ = dataset-specific constants (fitted from experimental data)\n",
    "\n",
    "For our e-commerce dataset, we fitted: Œ±=0.002, Œ≤=0.015, Œ≥=1.2 (R¬≤=0.94)\n",
    "\n",
    "[... continues ...]\n",
    "\n",
    "## 4. Implementation Recommendations\n",
    "\n",
    "Based on our findings, we recommend the following configuration for real-world deployments:\n",
    "\n",
    "```python\n",
    "# Optimal HNSW configuration for balanced performance\n",
    "index_params = {\n",
    "    \"M\": 32,                  # Balance recall and latency\n",
    "    \"ef_construction\": 200,   # Higher quality index\n",
    "    \"ef_runtime\": 100         # Fast search with good recall\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration achieves 0.94 recall with 4.3ms p95 latency, suitable for most real-time applications.\n",
    "\n",
    "## 5. Conclusion\n",
    "Our findings demonstrate that vector search optimization is fundamentally about understanding\n",
    "YOUR specific requirements and constraints. There is no one-size-fits-all configuration.\n",
    "\"\"\"\n",
    "\n",
    "paper_tokens = count_tokens(research_paper)\n",
    "print(f\"\"\"üìÑ Sample Research Paper\n",
    "{'=' * 80}\n",
    "Title: \"Optimizing Vector Search Performance in Redis\"\n",
    "\n",
    "Structure:\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background and Related Work\n",
    "- Performance Analysis and Results\n",
    "- Implementation Recommendations\n",
    "- Conclusion\n",
    "\n",
    "Token count: {paper_tokens:,}\n",
    "Word count: ~{len(research_paper.split())}\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6aa7e06337016528",
   "metadata": {},
   "source": [
    "### Analysis: Why This Research Paper NEEDS Chunking\n",
    "\n",
    "Let's compare the course catalog (doesn't need chunking) with the research paper (does need chunking):\n",
    "\n",
    "| Factor | Course Catalog | Research Paper |\n",
    "|--------|---------------|----------------|\n",
    "| **Document Structure** | Single topic per record | Multiple distinct sections |\n",
    "| **Semantic Completeness** | Each course is self-contained | Sections cover different topics |\n",
    "| **Query Patterns** | \"Show me CS courses\" | \"What compression techniques?\" |\n",
    "| **Optimal Retrieval Unit** | Whole course | Specific section |\n",
    "| **Token Count** | ~150-200 tokens | ~1,500+ tokens |\n",
    "| **Chunking Decision** | ‚ùå Don't chunk | ‚úÖ Chunk by section |\n",
    "\n",
    "**Why the research paper needs chunking:**\n",
    "\n",
    "**1. Multiple Distinct Topics:**\n",
    "- Abstract, Introduction, Background, Results, Conclusion each cover different aspects\n",
    "- A query about \"compression techniques\" only needs the relevant section, not the entire paper\n",
    "\n",
    "**2. Retrieval Precision:**\n",
    "- Without chunking: Retrieve entire 1,500-token paper for every query\n",
    "- With chunking: Retrieve only the 200-300 token section that's relevant\n",
    "- Result: 80% reduction in irrelevant context\n",
    "\n",
    "**3. Query-Specific Needs:**\n",
    "\n",
    "| Query | Needs | Without Chunking | With Chunking |\n",
    "|-------|-------|------------------|---------------|\n",
    "| \"What compression techniques?\" | Methodology section | Entire paper (1,500 tokens) | Methodology (300 tokens) |\n",
    "| \"What were recall results?\" | Results + Table | Entire paper (1,500 tokens) | Results section (250 tokens) |\n",
    "| \"How does HNSW work?\" | Background + Formula | Entire paper (1,500 tokens) | Background (200 tokens) |\n",
    "| \"Recommended config?\" | Implementation section | Entire paper (1,500 tokens) | Implementation (150 tokens) |\n",
    "\n",
    "**Impact:** 5-10x reduction in irrelevant context, leading to faster responses and better quality.\n",
    "\n",
    "**üí° Key Insight:** Chunking isn't about fitting in context windows - it's about **data modeling for retrieval**. Just like you wouldn't store all customer data in one database row, you shouldn't embed all document content in one vector when sections serve different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d079e72743b37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Research Background - Why Chunking Matters\n",
    "\n",
    "Even with large context windows (128K+ tokens), research shows that **how you structure context matters more than fitting everything in**.\n",
    "\n",
    "### Key Research Findings\n",
    "\n",
    "**1. \"Lost in the Middle\" (Stanford/UC Berkeley, 2023)**\n",
    "\n",
    "*Source: [arXiv:2307.03172](https://arxiv.org/abs/2307.03172)*\n",
    "\n",
    "- LLMs exhibit **U-shaped attention**: high recall at beginning/end, degraded in middle\n",
    "- Happens even in models designed for long contexts\n",
    "- **Implication:** Chunking ensures relevant sections are retrieved and placed prominently, not buried\n",
    "\n",
    "**2. \"Context Rot\" (Chroma Research, 2025)**\n",
    "\n",
    "*Source: [research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)*\n",
    "\n",
    "- Performance degrades as input length increases, even when relevant info is present\n",
    "- **Distractor effect**: Irrelevant content actively hurts model performance\n",
    "- Even 4 distractor documents can significantly degrade output quality\n",
    "- **Implication:** Smaller, focused chunks reduce \"distractor tokens\"\n",
    "\n",
    "**3. Needle in the Haystack (NIAH) Benchmark**\n",
    "\n",
    "*Source: [github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)*\n",
    "\n",
    "- Models often fail to retrieve information buried in long context\n",
    "- Performance varies by position (middle is worst)\n",
    "- **Limitation:** Tests lexical retrieval only, not semantic understanding\n",
    "- **Implication:** For structured data, NIAH is irrelevant‚Äîeach record IS the needle\n",
    "\n",
    "**The Key Insight:**\n",
    "\n",
    "These findings inform design decisions but don't prescribe universal rules:\n",
    "\n",
    "- **Structured records** (courses, products, FAQs): \"Lost in the middle\" doesn't apply‚Äîeach record is already focused\n",
    "- **Long-form documents** (papers, books): Context rot and positional bias become relevant‚Äîchunking helps\n",
    "- **Mixed content**: Real-world data rarely fits neat categories‚Äîexperiment with YOUR data\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Chunking Strategies - Three Approaches\n",
    "\n",
    "Once you've determined your data needs chunking, the next question is: **How should you chunk it?**\n",
    "\n",
    "There's no single \"best\" strategy - the optimal approach depends on YOUR data characteristics and query patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14864605b206341",
   "metadata": {},
   "source": [
    "### Strategy 1: Document-Based Chunking (Structure-Aware)\n",
    "\n",
    "**Concept:** Split documents based on their inherent structure (sections, paragraphs, headings).\n",
    "\n",
    "**Best for:** Structured documents with clear logical divisions (research papers, technical docs, books)."
   ]
  },
  {
   "cell_type": "code",
   "id": "2f523c504991979f",
   "metadata": {},
   "source": [
    "# Strategy 1: Document-Based Chunking\n",
    "# Split research paper by sections (using markdown headers)\n",
    "\n",
    "\n",
    "def chunk_by_structure(text: str, separator: str = \"\\n## \") -> List[str]:\n",
    "    \"\"\"Split text by structural markers (e.g., markdown headers).\"\"\"\n",
    "\n",
    "    # Split by headers\n",
    "    sections = text.split(separator)\n",
    "\n",
    "    # Clean and format chunks\n",
    "    chunks = []\n",
    "    for i, section in enumerate(sections):\n",
    "        if section.strip():\n",
    "            # Add header back (except for first chunk which is title)\n",
    "            if i > 0:\n",
    "                chunk = \"## \" + section\n",
    "            else:\n",
    "                chunk = section\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply to research paper\n",
    "structure_chunks = chunk_by_structure(research_paper)\n",
    "\n",
    "print(f\"\"\"üìä Strategy 1: Document-Based (Structure-Aware) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(structure_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(structure_chunks):\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    # Show first 100 chars of each chunk\n",
    "    preview = chunk[:300].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "108caf4be1a606b4",
   "metadata": {},
   "source": [
    "**Strategy 1 Analysis:**\n",
    "\n",
    "‚úÖ **Advantages:**\n",
    "- Respects document structure (sections stay together)\n",
    "- Semantically coherent (each chunk is a complete section)\n",
    "- Easy to implement for structured documents\n",
    "- **Keeps tables, formulas, and code WITH their context**\n",
    "\n",
    "‚ö†Ô∏è **Trade-offs:**\n",
    "- Variable chunk sizes (some sections longer than others)\n",
    "- Requires documents to have clear structure\n",
    "- May create chunks that are still too large\n",
    "\n",
    "üéØ **Best for:**\n",
    "- Research papers with clear sections\n",
    "- Technical documentation with headers\n",
    "- Books with chapters/sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca013174da787352",
   "metadata": {},
   "source": [
    "### Strategy 2: Fixed-Size Chunking (Token-Based)\n",
    "\n",
    "**Concept:** Split text into chunks of a predetermined size (e.g., 512 tokens) with overlap.\n",
    "\n",
    "**Best for:** Unstructured text, quick prototyping, when you need consistent chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "id": "3f70130fcdc66f34",
   "metadata": {},
   "source": [
    "# Strategy 2: Fixed-Size Chunking (Using LangChain)\n",
    "# Industry-standard approach with smart boundary detection\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter with smart boundary detection\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # Target chunk size in characters\n",
    "    chunk_overlap=100,  # Overlap to preserve context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Try these in order\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "print(\"üîÑ Running fixed-size chunking with LangChain...\")\n",
    "print(\"   Trying to split on: paragraphs ‚Üí sentences ‚Üí words ‚Üí characters\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "fixed_chunks_docs = text_splitter.create_documents([research_paper])\n",
    "fixed_chunks = [doc.page_content for doc in fixed_chunks_docs]\n",
    "\n",
    "print(f\"\"\"üìä Strategy 2: Fixed-Size (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Target chunk size: 800 characters (~200 words)\n",
    "Overlap: 100 characters\n",
    "Number of chunks: {len(fixed_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(fixed_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "print(f\"... ({len(fixed_chunks) - 5} more chunks)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4afda8cda87c128",
   "metadata": {},
   "source": [
    "**Strategy 2 Analysis:**\n",
    "\n",
    "‚úÖ **Advantages:**\n",
    "- **Respects natural boundaries**: Tries paragraphs ‚Üí sentences ‚Üí words ‚Üí characters\n",
    "- Consistent chunk sizes (predictable token usage)\n",
    "- Works on any text (structured or unstructured)\n",
    "- **Doesn't split mid-sentence** (unless absolutely necessary)\n",
    "\n",
    "‚ö†Ô∏è **Trade-offs:**\n",
    "- Ignores document structure (doesn't understand sections)\n",
    "- Can break semantic coherence (may split related content)\n",
    "- Overlap creates redundancy (increases storage/cost)\n",
    "\n",
    "üéØ **Best for:**\n",
    "- Unstructured text (no clear sections)\n",
    "- Quick prototyping and baselines\n",
    "- When consistent chunk sizes are required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d3ece2f9f06b8",
   "metadata": {},
   "source": [
    "### Strategy 3: Semantic Chunking (Meaning-Based)\n",
    "\n",
    "**Concept:** Split text based on semantic similarity using embeddings - create new chunks when topic changes significantly.\n",
    "\n",
    "**How it works:**\n",
    "1. Split text into sentences or paragraphs\n",
    "2. Generate embeddings for each segment\n",
    "3. Calculate similarity between consecutive segments\n",
    "4. Create chunk boundaries where similarity drops (topic shift detected)\n",
    "\n",
    "**Best for:** Dense academic text, legal documents, narratives where semantic boundaries don't align with structure."
   ]
  },
  {
   "cell_type": "code",
   "id": "7b01f496f5384cbe",
   "metadata": {},
   "source": [
    "# Strategy 3: Semantic Chunking (Using LangChain)\n",
    "# Industry-standard approach with local embeddings (no API costs!)\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize local embeddings (no API costs!)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# Create semantic chunker with percentile-based breakpoint detection\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",  # Split at bottom 25% of similarities\n",
    "    breakpoint_threshold_amount=25,  # 25th percentile\n",
    "    buffer_size=1,  # Compare consecutive sentences\n",
    ")\n",
    "\n",
    "print(\"üîÑ Running semantic chunking with LangChain...\")\n",
    "print(\"   Using local embeddings (sentence-transformers/all-MiniLM-L6-v2)\")\n",
    "print(\"   Breakpoint detection: 25th percentile of similarity scores\\n\")\n",
    "\n",
    "# Apply to research paper\n",
    "semantic_chunks_docs = semantic_chunker.create_documents([research_paper])\n",
    "\n",
    "# Extract text from Document objects\n",
    "semantic_chunks = [doc.page_content for doc in semantic_chunks_docs]\n",
    "\n",
    "print(f\"\"\"üìä Strategy 3: Semantic (LangChain) Chunking\n",
    "{'=' * 80}\n",
    "Original document: {paper_tokens:,} tokens\n",
    "Number of chunks: {len(semantic_chunks)}\n",
    "\n",
    "Chunk breakdown:\n",
    "\"\"\")\n",
    "\n",
    "for i, chunk in enumerate(semantic_chunks[:5]):  # Show first 5\n",
    "    chunk_tokens = count_tokens(chunk)\n",
    "    preview = chunk[:100].replace(\"\\n\", \" \")\n",
    "    print(f\"   Chunk {i+1}: {chunk_tokens:,} tokens - {preview}...\")\n",
    "\n",
    "if len(semantic_chunks) > 5:\n",
    "    print(f\"... ({len(semantic_chunks) - 5} more chunks)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c39c7fe9faa9a03",
   "metadata": {},
   "source": [
    "**Strategy 3 Analysis:**\n",
    "\n",
    "‚úÖ **Advantages:**\n",
    "- **Meaning-aware**: Chunks based on topic shifts, not arbitrary boundaries\n",
    "- **Adaptive**: Chunk sizes vary based on content coherence\n",
    "- **Better retrieval**: Each chunk is semantically focused\n",
    "- **Free**: Uses local embeddings (no API costs)\n",
    "\n",
    "‚ö†Ô∏è **Trade-offs:**\n",
    "- Slower processing (requires embedding generation)\n",
    "- Variable chunk sizes (harder to predict token usage)\n",
    "- May not respect document structure (sections, headers)\n",
    "- Requires tuning (threshold, buffer size)\n",
    "\n",
    "üéØ **Best for:**\n",
    "- Dense academic text\n",
    "- Legal documents\n",
    "- Narratives and stories\n",
    "- Content where semantic boundaries don't align with structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d0828e8512bd8",
   "metadata": {},
   "source": [
    "### Comparing Chunking Strategies: Decision Framework\n",
    "\n",
    "Now let's compare all strategies side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "id": "399cd8dfad49c0af",
   "metadata": {},
   "source": [
    "print(f\"\"\"\n",
    "{'=' * 80}\n",
    "CHUNKING STRATEGY COMPARISON\n",
    "{'=' * 80}\n",
    "\n",
    "Document: Research Paper ({paper_tokens:,} tokens)\n",
    "\n",
    "Strategy              | Chunks | Avg Size | Complexity | Best For\n",
    "--------------------- | ------ | -------- | ---------- | --------\n",
    "Document-Based        | {len(structure_chunks):>6} | {sum(count_tokens(c) for c in structure_chunks) // len(structure_chunks):>8} | Low        | Structured docs\n",
    "Fixed-Size            | {len(fixed_chunks):>6} | {sum(count_tokens(c) for c in fixed_chunks) // len(fixed_chunks):>8} | Low        | Unstructured text\n",
    "Semantic              | {len(semantic_chunks):>6} | {sum(count_tokens(c) for c in semantic_chunks) // len(semantic_chunks):>8} | High       | Dense academic text\n",
    "\n",
    "{'=' * 80}\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11c058c97c25c9b3",
   "metadata": {},
   "source": [
    "### YOUR Chunking Decision Framework\n",
    "\n",
    "Chunking strategy is a **design choice** that depends on your specific context. There's no universal \"correct\" chunk size.\n",
    "\n",
    "**Step 1: Start with Document Type**\n",
    "\n",
    "| Document Type | Default Approach | Reasoning |\n",
    "|---------------|------------------|----------|\n",
    "| **Structured records** (courses, products, FAQs) | Don't chunk | Natural boundaries already exist |\n",
    "| **Long-form text** (papers, books, docs) | Consider chunking | May need retrieval precision |\n",
    "| **PDFs with visual layout** | Page-level | Preserves tables, figures |\n",
    "| **Code** | Function/class boundaries | Semantic structure matters |\n",
    "\n",
    "**Step 2: Evaluate These Factors**\n",
    "\n",
    "1. **Semantic completeness:** Is each item self-contained?\n",
    "   - ‚úÖ Yes ‚Üí Don't chunk (preserve natural boundaries)\n",
    "   - ‚ùå No ‚Üí Consider chunking strategy\n",
    "\n",
    "2. **Query patterns:** What will users ask?\n",
    "   - Specific facts ‚Üí Smaller, focused chunks help\n",
    "   - Summaries/overviews ‚Üí Larger chunks or hierarchical\n",
    "   - Mixed ‚Üí Consider hierarchical approach\n",
    "\n",
    "3. **Topic density:** How many distinct topics per document?\n",
    "   - Single topic ‚Üí Whole-document embedding often works\n",
    "   - Multiple distinct topics ‚Üí Chunking may improve precision\n",
    "\n",
    "**Example Decisions:**\n",
    "\n",
    "| Domain | Data Characteristics | Decision | Why |\n",
    "|--------|---------------------|----------|-----|\n",
    "| **Course Catalog** | Small, self-contained records | **Don't chunk** | Each course is a complete retrieval unit |\n",
    "| **Research Papers** | Multi-section, dense topics | Document-Based | Sections are natural semantic units |\n",
    "| **Support Tickets** | Single issue per ticket | **Don't chunk** | Already at optimal granularity |\n",
    "| **Legal Contracts** | Nested structure, many clauses | Hierarchical | Need both overview and clause-level detail |\n",
    "\n",
    "> üí° **Key Takeaway:** Ask \"What is my natural retrieval unit?\" before deciding on a chunking strategy. For many structured data use cases, the answer is \"don't chunk.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fde7b8f9ea7523",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Data Modeling is the Foundation of RAG Quality**\n",
    "- The critical first question: \"What is my natural retrieval unit?\"\n",
    "- For structured records (courses, products, FAQs), the answer is often \"don't chunk\"\n",
    "- For long-form documents (papers, books), chunking may improve retrieval precision\n",
    "\n",
    "**2. The \"Don't Chunk\" Strategy is Valid**\n",
    "- Course catalogs, product listings, FAQ entries don't need chunking\n",
    "- Each record is already semantically complete and self-contained\n",
    "- Chunking would fragment related information and hurt quality\n",
    "- Use hierarchical patterns (summaries + details) instead\n",
    "\n",
    "**3. When Chunking Helps**\n",
    "- Long-form documents with multiple distinct topics\n",
    "- Research papers, technical documentation, books, legal contracts\n",
    "- Improves retrieval precision by reducing irrelevant context\n",
    "- Research-backed: \"Lost in the Middle\", \"Context Rot\" show why structure matters\n",
    "\n",
    "**4. Three Chunking Strategies**\n",
    "- **Document-Based (Structure-Aware):** Split by sections/headers - best for structured documents\n",
    "- **Fixed-Size (Token-Based):** Split into fixed chunks with overlap - best for unstructured text\n",
    "- **Semantic (Meaning-Based):** Split based on topic shifts - best for dense academic text\n",
    "- Choose based on YOUR data characteristics and query patterns\n",
    "\n",
    "**5. The Engineering Mindset**\n",
    "- Chunking is a design choice, not a default step\n",
    "- Like database schema design, structure affects retrieval quality\n",
    "- No one-size-fits-all solution - analyze YOUR data and requirements\n",
    "- Experiment, measure, iterate\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "**Ask these questions:**\n",
    "\n",
    "1. **What is my natural retrieval unit?**\n",
    "   - Single record (course, product) ‚Üí Don't chunk\n",
    "   - Document section (paper, book) ‚Üí Consider chunking\n",
    "\n",
    "2. **What are my query patterns?**\n",
    "   - \"Show me CS courses\" ‚Üí Whole-record embedding\n",
    "   - \"What compression techniques?\" ‚Üí Section-level chunking\n",
    "\n",
    "3. **How many distinct topics per document?**\n",
    "   - Single topic ‚Üí Whole-document embedding\n",
    "   - Multiple topics ‚Üí Chunking improves precision\n",
    "\n",
    "**Example Decisions:**\n",
    "\n",
    "| Domain | Data Type | Decision | Strategy |\n",
    "|--------|-----------|----------|----------|\n",
    "| **Course Catalog** | Structured records | Don't chunk | Hierarchical (summaries + details) |\n",
    "| **Research Papers** | Multi-section documents | Chunk | Document-based (by section) |\n",
    "| **Support Tickets** | Single-issue records | Don't chunk | Whole-record embedding |\n",
    "| **Legal Contracts** | Multi-clause documents | Chunk | Hierarchical + document-based |\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "> **Chunking isn't about fitting in context windows - it's about data modeling for retrieval.**\n",
    ">\n",
    "> Just like you wouldn't store all customer data in one database row, you shouldn't embed all document content in one vector without thinking about retrieval patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "### Module 4: Memory Systems for Context Engineering\n",
    "\n",
    "Now that you understand data modeling and chunking for knowledge bases, you'll learn to manage conversation context:\n",
    "- **Working Memory:** Track conversation history within a session\n",
    "- **Long-term Memory:** Remember user preferences across sessions\n",
    "- **Memory-Enhanced RAG:** Combine retrieved knowledge with conversation memory\n",
    "- **Redis Agent Memory Server:** Automatic memory extraction and retrieval\n",
    "\n",
    "```\n",
    "Module 1: Context Engineering Fundamentals\n",
    "    ‚Üì\n",
    "Module 2: RAG Fundamentals ‚Üê Completed\n",
    "    ‚Üì\n",
    "Module 3: Chunking and Data Modeling ‚Üê You are here\n",
    "    ‚Üì\n",
    "Module 4: Memory Systems ‚Üê Next\n",
    "    ‚Üì\n",
    "Module 5: Building Agents (Complete System)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Analyze Your Data\n",
    "Think about a dataset you work with. Answer these questions:\n",
    "1. What is the natural retrieval unit?\n",
    "2. Does it need chunking? Why or why not?\n",
    "3. If yes, which chunking strategy would you use?\n",
    "\n",
    "### Exercise 2: Design a Chunking Strategy\n",
    "For each document type, choose the best approach:\n",
    "1. Product catalog with 1,000 items\n",
    "2. 50-page technical manual with chapters\n",
    "3. Customer support tickets (avg 200 words each)\n",
    "4. Legal contracts (avg 20 pages, multiple clauses)\n",
    "\n",
    "### Exercise 3: Experiment with Chunking\n",
    "Take the research paper example and:\n",
    "1. Try all three chunking strategies\n",
    "2. Compare the number of chunks and average size\n",
    "3. Which strategy would work best for queries about \"HNSW configuration\"?\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "**Chunking Strategies:**\n",
    "- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "- [LlamaIndex Node Parsers](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/)\n",
    "\n",
    "**Research Papers:**\n",
    "- [\"Lost in the Middle\" (arXiv:2307.03172)](https://arxiv.org/abs/2307.03172)\n",
    "- [\"Context Rot\" (Chroma Research)](https://research.trychroma.com/context-rot)\n",
    "- [Needle in the Haystack Benchmark](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)\n",
    "\n",
    "**Data Modeling for RAG:**\n",
    "- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "\n",
    "**Vector Databases:**\n",
    "- [Redis Vector Search Documentation](https://redis.io/docs/stack/search/reference/vectors/)\n",
    "- [RedisVL Python Library](https://github.com/RedisVentures/redisvl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
